{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 6596\n",
      "Evaluation size:  1650\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('data/negative_labr.txt', 'r') as data:\n",
    "    dataset = [\"<|review|>\" + x.strip() for x in data.readlines()]\n",
    "\n",
    "train, val = train_test_split(dataset, train_size=.8, random_state=2020)\n",
    "print(\"training size:\" , len(train))\n",
    "print(\"Evaluation size: \" , len(val))\n",
    "\n",
    "with open('data/train_tmp.txt', 'w') as file_handle:\n",
    "    file_handle.write(\"<|endoftext|>\".join(train))\n",
    "\n",
    "with open('data/eval_tmp.txt', 'w') as file_handle:\n",
    "    file_handle.write(\"<|endoftext|>\".join(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar_corpus\t\t\t    GPT2-SentenceGenerator.ipynb\r\n",
      "ar_corpus.tar.xz\t\t    init.txt\r\n",
      "data\t\t\t\t    pre-proccessing.ipynb\r\n",
      "eval_tmp.txt\t\t\t    reviews.tsv\r\n",
      "fine-tuning-gpt-2.ipynb\t\t    test.ipynb\r\n",
      "gan.ipynb\t\t\t    tokenized_data\r\n",
      "gpt2-arabic-sentence-generator\t    train.ipynb\r\n",
      "gpt2-arabic-sentence-generator.zip  train_tmp.txt\r\n",
      "gpt-2-negative-reviews\t\t    transformers\r\n",
      "GPT2Sentence\t\t\t    wiki_download.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  8 23:22:51 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 207...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8    11W /  N/A |    462MiB /  7973MiB |     16%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1586      G   /usr/lib/xorg/Xorg                 59MiB |\n",
      "|    0   N/A  N/A      3326      G   /usr/lib/xorg/Xorg                121MiB |\n",
      "|    0   N/A  N/A      3460      G   /usr/bin/gnome-shell              104MiB |\n",
      "|    0   N/A  N/A      4920      G   ...AAAAAAAAA= --shared-files      162MiB |\n",
      "|    0   N/A  N/A     13761      G   /usr/bin/nvidia-settings            0MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t       run_clm.py\t      run_mlm.py    transformers\r\n",
      "requirements.txt       run_mlm_flax.py\t      run_plm.py\r\n",
      "run_clm_no_trainer.py  run_mlm_no_trainer.py  saved_models\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"transformers\")\n",
    "os.chdir(\"./examples/language-modeling\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets>=1.1.3 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.1.95)\n",
      "Requirement already satisfied: protobuf in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.15.8)\n",
      "Requirement already satisfied: multiprocess in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\n",
      "Requirement already satisfied: fsspec in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: dill in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.2)\n",
      "Requirement already satisfied: pandas in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.2.3)\n",
      "Requirement already satisfied: xxhash in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: filelock in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: six>=1.9 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t       run_clm.py\t      run_mlm.py    transformers\r\n",
      "requirements.txt       run_mlm_flax.py\t      run_plm.py\r\n",
      "run_clm_no_trainer.py  run_mlm_no_trainer.py  saved_models\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from pyarrow) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/huggingface/transformers/\n",
      "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-5480jez5\n",
      "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-5480jez5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (4.49.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (2021.4.4)\n",
      "Requirement already satisfied: requests in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (1.19.2)\n",
      "Requirement already satisfied: sacremoses in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (0.0.44)\n",
      "Requirement already satisfied: filelock in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (2.0.0)\n",
      "Requirement already satisfied: packaging in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from transformers==4.6.0.dev0) (20.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from importlib-metadata->transformers==4.6.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from packaging->transformers==4.6.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from packaging->transformers==4.6.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from requests->transformers==4.6.0.dev0) (2.10)\n",
      "Requirement already satisfied: joblib in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from sacremoses->transformers==4.6.0.dev0) (0.17.0)\n",
      "Requirement already satisfied: click in /home/mofawzy/anaconda3/envs/gpt-2/lib/python3.7/site-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+git://github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../../data/\"\n",
    "TRAIN_FILE = DATA_DIR+\"train_tmp.txt\"\n",
    "EVAL_FILE = DATA_DIR+\"eval_tmp.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/08/2021 23:29:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "04/08/2021 23:29:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=saved_models, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Apr08_23-29-59_mofawzy, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=saved_models, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n",
      "04/08/2021 23:30:00 - WARNING - datasets.builder -   Using custom data configuration default-666c580b2e09d4f8\n",
      "04/08/2021 23:30:00 - WARNING - datasets.builder -   Reusing dataset text (/home/mofawzy/.cache/huggingface/datasets/text/default-666c580b2e09d4f8/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "[INFO|configuration_utils.py:490] 2021-04-08 23:30:01,502 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/mofawzy/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:526] 2021-04-08 23:30:01,504 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:490] 2021-04-08 23:30:02,314 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/mofawzy/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:526] 2021-04-08 23:30:02,315 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,458 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /home/mofawzy/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,459 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /home/mofawzy/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,459 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /home/mofawzy/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,459 >> loading file https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,459 >> loading file https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1713] 2021-04-08 23:30:07,459 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1069] 2021-04-08 23:30:08,256 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /home/mofawzy/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1198] 2021-04-08 23:30:17,322 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1207] 2021-04-08 23:30:17,322 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3143] 2021-04-08 23:30:19,544 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2144388 > 1024). Running this sequence through the model will result in indexing errors\n",
      "04/08/2021 23:30:19 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/mofawzy/.cache/huggingface/datasets/text/default-666c580b2e09d4f8/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-ed7a00856895048c.arrow\n",
      "04/08/2021 23:30:20 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/mofawzy/.cache/huggingface/datasets/text/default-666c580b2e09d4f8/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-d70ec1c5d097c5e0.arrow\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.98s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.24ba/s]\n",
      "[INFO|trainer.py:402] 2021-04-08 23:30:26,350 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:628] 2021-04-08 23:30:26,351 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:628] 2021-04-08 23:30:26,356 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1013] 2021-04-08 23:30:26,356 >> ***** Running training *****\n",
      "[INFO|trainer.py:1014] 2021-04-08 23:30:26,356 >>   Num examples = 178699\n",
      "[INFO|trainer.py:1015] 2021-04-08 23:30:26,356 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1016] 2021-04-08 23:30:26,356 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1017] 2021-04-08 23:30:26,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1018] 2021-04-08 23:30:26,356 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1019] 2021-04-08 23:30:26,356 >>   Total optimization steps = 893495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7546, 'learning_rate': 4.997235574905288e-05, 'epoch': 0.0}          \n",
      "{'loss': 2.5508, 'learning_rate': 4.9944375737972794e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.5424, 'learning_rate': 4.991639572689271e-05, 'epoch': 0.01}         \n",
      "{'loss': 2.5188, 'learning_rate': 4.9888415715812625e-05, 'epoch': 0.01}        \n",
      "{'loss': 2.4294, 'learning_rate': 4.98604916647547e-05, 'epoch': 0.01}          \n",
      "{'loss': 2.4129, 'learning_rate': 4.9832511653674615e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.4272, 'learning_rate': 4.980453164259453e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.3474, 'learning_rate': 4.977655163151445e-05, 'epoch': 0.02}         \n",
      "{'loss': 2.2996, 'learning_rate': 4.974857162043436e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.2878, 'learning_rate': 4.972059160935428e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.3134, 'learning_rate': 4.96926115982742e-05, 'epoch': 0.03}          \n",
      "{'loss': 2.3096, 'learning_rate': 4.966463158719411e-05, 'epoch': 0.03}         \n",
      "{'loss': 2.3273, 'learning_rate': 4.9636651576114024e-05, 'epoch': 0.04}        \n",
      "{'loss': 2.3125, 'learning_rate': 4.960867156503394e-05, 'epoch': 0.04}         \n",
      "{'loss': 2.3176, 'learning_rate': 4.9580691553953855e-05, 'epoch': 0.04}        \n",
      "{'loss': 2.2951, 'learning_rate': 4.955271154287378e-05, 'epoch': 0.04}         \n",
      "{'loss': 2.2389, 'learning_rate': 4.9524731531793686e-05, 'epoch': 0.05}        \n",
      "{'loss': 2.3015, 'learning_rate': 4.949686344075793e-05, 'epoch': 0.05}         \n",
      "{'loss': 2.2669, 'learning_rate': 4.946888342967784e-05, 'epoch': 0.05}         \n",
      "{'loss': 2.2653, 'learning_rate': 4.944090341859776e-05, 'epoch': 0.06}         \n",
      "{'loss': 2.2592, 'learning_rate': 4.941292340751767e-05, 'epoch': 0.06}         \n",
      "{'loss': 2.2344, 'learning_rate': 4.938494339643759e-05, 'epoch': 0.06}         \n",
      "{'loss': 2.2026, 'learning_rate': 4.9356963385357505e-05, 'epoch': 0.06}        \n",
      "{'loss': 2.2061, 'learning_rate': 4.932903933429958e-05, 'epoch': 0.07}         \n",
      "{'loss': 2.2209, 'learning_rate': 4.9301059323219495e-05, 'epoch': 0.07}        \n",
      "{'loss': 2.1965, 'learning_rate': 4.927307931213941e-05, 'epoch': 0.07}         \n",
      "{'loss': 2.2177, 'learning_rate': 4.9245099301059326e-05, 'epoch': 0.08}        \n",
      "{'loss': 2.2174, 'learning_rate': 4.921711928997924e-05, 'epoch': 0.08}         \n",
      "{'loss': 2.1312, 'learning_rate': 4.918913927889916e-05, 'epoch': 0.08}         \n",
      "{'loss': 2.1579, 'learning_rate': 4.916115926781907e-05, 'epoch': 0.08}         \n",
      "{'loss': 2.1893, 'learning_rate': 4.913317925673899e-05, 'epoch': 0.09}         \n",
      "{'loss': 2.1632, 'learning_rate': 4.9105199245658904e-05, 'epoch': 0.09}        \n",
      "{'loss': 2.1519, 'learning_rate': 4.907727519460098e-05, 'epoch': 0.09}         \n",
      "{'loss': 2.1507, 'learning_rate': 4.9049295183520894e-05, 'epoch': 0.1}         \n",
      "{'loss': 2.1854, 'learning_rate': 4.902131517244081e-05, 'epoch': 0.1}          \n",
      "{'loss': 2.1787, 'learning_rate': 4.8993335161360725e-05, 'epoch': 0.1}         \n",
      "{'loss': 2.1741, 'learning_rate': 4.89654111103028e-05, 'epoch': 0.1}           \n",
      "{'loss': 2.2028, 'learning_rate': 4.893743109922272e-05, 'epoch': 0.11}         \n",
      "{'loss': 2.1778, 'learning_rate': 4.890945108814263e-05, 'epoch': 0.11}         \n",
      "{'loss': 2.1828, 'learning_rate': 4.8881527037084705e-05, 'epoch': 0.11}        \n",
      "{'loss': 2.1671, 'learning_rate': 4.885354702600463e-05, 'epoch': 0.11}         \n",
      "{'loss': 2.1133, 'learning_rate': 4.8825567014924536e-05, 'epoch': 0.12}        \n",
      "{'loss': 2.1726, 'learning_rate': 4.879758700384446e-05, 'epoch': 0.12}         \n",
      "{'loss': 2.1438, 'learning_rate': 4.8769606992764374e-05, 'epoch': 0.12}        \n",
      "{'loss': 2.1314, 'learning_rate': 4.874162698168428e-05, 'epoch': 0.13}         \n",
      "{'loss': 2.1493, 'learning_rate': 4.8713646970604205e-05, 'epoch': 0.13}        \n",
      "{'loss': 2.1107, 'learning_rate': 4.868566695952412e-05, 'epoch': 0.13}         \n",
      "{'loss': 2.1472, 'learning_rate': 4.865774290846619e-05, 'epoch': 0.13}         \n",
      "{'loss': 2.1245, 'learning_rate': 4.862976289738611e-05, 'epoch': 0.14}         \n",
      "{'loss': 2.1205, 'learning_rate': 4.8601782886306026e-05, 'epoch': 0.14}        \n",
      "{'loss': 2.1088, 'learning_rate': 4.857380287522594e-05, 'epoch': 0.14}         \n",
      "{'loss': 2.1384, 'learning_rate': 4.854582286414586e-05, 'epoch': 0.15}         \n",
      "{'loss': 2.1053, 'learning_rate': 4.851784285306577e-05, 'epoch': 0.15}         \n",
      "{'loss': 2.1244, 'learning_rate': 4.848986284198569e-05, 'epoch': 0.15}         \n",
      "{'loss': 2.1043, 'learning_rate': 4.8461882830905604e-05, 'epoch': 0.15}        \n",
      "{'loss': 2.1314, 'learning_rate': 4.843390281982551e-05, 'epoch': 0.16}         \n",
      "{'loss': 2.1085, 'learning_rate': 4.8405978768767594e-05, 'epoch': 0.16}        \n",
      "{'loss': 2.0949, 'learning_rate': 4.837799875768751e-05, 'epoch': 0.16}         \n",
      "{'loss': 2.1059, 'learning_rate': 4.8350074706629584e-05, 'epoch': 0.17}        \n",
      "{'loss': 2.1815, 'learning_rate': 4.83220946955495e-05, 'epoch': 0.17}          \n",
      "{'loss': 2.0799, 'learning_rate': 4.8294114684469416e-05, 'epoch': 0.17}        \n",
      "{'loss': 2.0781, 'learning_rate': 4.82661906334115e-05, 'epoch': 0.17}          \n",
      "{'loss': 2.0939, 'learning_rate': 4.8238210622331406e-05, 'epoch': 0.18}        \n",
      "{'loss': 2.0939, 'learning_rate': 4.821023061125132e-05, 'epoch': 0.18}         \n",
      "{'loss': 2.0883, 'learning_rate': 4.8182250600171244e-05, 'epoch': 0.18}        \n",
      "{'loss': 2.0844, 'learning_rate': 4.815427058909115e-05, 'epoch': 0.18}         \n",
      "{'loss': 2.072, 'learning_rate': 4.812629057801107e-05, 'epoch': 0.19}          \n",
      "{'loss': 2.0597, 'learning_rate': 4.809831056693099e-05, 'epoch': 0.19}         \n",
      "{'loss': 2.0869, 'learning_rate': 4.80703305558509e-05, 'epoch': 0.19}          \n",
      "{'loss': 2.0658, 'learning_rate': 4.804240650479298e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.085, 'learning_rate': 4.8014426493712896e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.124, 'learning_rate': 4.7986446482632805e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.1029, 'learning_rate': 4.795846647155273e-05, 'epoch': 0.2}          \n",
      "{'loss': 2.0835, 'learning_rate': 4.793048646047264e-05, 'epoch': 0.21}         \n",
      "{'loss': 2.0721, 'learning_rate': 4.790250644939255e-05, 'epoch': 0.21}         \n",
      "{'loss': 2.0829, 'learning_rate': 4.7874526438312474e-05, 'epoch': 0.21}        \n",
      "{'loss': 2.0467, 'learning_rate': 4.784660238725455e-05, 'epoch': 0.22}         \n",
      "{'loss': 2.0892, 'learning_rate': 4.7818622376174464e-05, 'epoch': 0.22}        \n",
      "{'loss': 2.0382, 'learning_rate': 4.779069832511654e-05, 'epoch': 0.22}         \n",
      "{'loss': 2.1157, 'learning_rate': 4.7762718314036454e-05, 'epoch': 0.22}        \n",
      "{'loss': 2.0085, 'learning_rate': 4.773473830295637e-05, 'epoch': 0.23}         \n",
      "{'loss': 2.0612, 'learning_rate': 4.7706758291876285e-05, 'epoch': 0.23}        \n",
      "{'loss': 2.0774, 'learning_rate': 4.76787782807962e-05, 'epoch': 0.23}          \n",
      "{'loss': 2.062, 'learning_rate': 4.7650798269716116e-05, 'epoch': 0.24}         \n",
      "{'loss': 2.0908, 'learning_rate': 4.762281825863603e-05, 'epoch': 0.24}         \n",
      "{'loss': 2.0597, 'learning_rate': 4.759483824755595e-05, 'epoch': 0.24}         \n",
      "{'loss': 2.0503, 'learning_rate': 4.756691419649802e-05, 'epoch': 0.24}         \n",
      "{'loss': 2.0619, 'learning_rate': 4.753893418541794e-05, 'epoch': 0.25}         \n",
      "{'loss': 2.0313, 'learning_rate': 4.751095417433786e-05, 'epoch': 0.25}         \n",
      "{'loss': 2.0743, 'learning_rate': 4.748297416325777e-05, 'epoch': 0.25}         \n",
      "{'loss': 2.0447, 'learning_rate': 4.7454994152177684e-05, 'epoch': 0.25}        \n",
      "{'loss': 2.0443, 'learning_rate': 4.7427070101119766e-05, 'epoch': 0.26}        \n",
      "{'loss': 2.0924, 'learning_rate': 4.7399090090039674e-05, 'epoch': 0.26}        \n",
      "{'loss': 2.0991, 'learning_rate': 4.73711100789596e-05, 'epoch': 0.26}          \n",
      "{'loss': 2.0423, 'learning_rate': 4.734313006787951e-05, 'epoch': 0.27}         \n",
      "{'loss': 2.0241, 'learning_rate': 4.731515005679942e-05, 'epoch': 0.27}         \n",
      "{'loss': 2.0571, 'learning_rate': 4.72872260057415e-05, 'epoch': 0.27}          \n",
      "{'loss': 2.063, 'learning_rate': 4.725924599466142e-05, 'epoch': 0.27}          \n",
      "{'loss': 2.0935, 'learning_rate': 4.723126598358133e-05, 'epoch': 0.28}         \n",
      "{'loss': 2.0472, 'learning_rate': 4.720328597250125e-05, 'epoch': 0.28}         \n",
      "{'loss': 2.0412, 'learning_rate': 4.7175305961421165e-05, 'epoch': 0.28}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0765, 'learning_rate': 4.714732595034108e-05, 'epoch': 0.29}         \n",
      "{'loss': 2.0498, 'learning_rate': 4.7119345939260996e-05, 'epoch': 0.29}        \n",
      "{'loss': 2.0202, 'learning_rate': 4.709136592818091e-05, 'epoch': 0.29}         \n",
      "{'loss': 2.0415, 'learning_rate': 4.706338591710083e-05, 'epoch': 0.29}         \n",
      "{'loss': 2.0468, 'learning_rate': 4.70354618660429e-05, 'epoch': 0.3}           \n",
      "{'loss': 2.0686, 'learning_rate': 4.700748185496282e-05, 'epoch': 0.3}          \n",
      "{'loss': 2.0599, 'learning_rate': 4.697950184388273e-05, 'epoch': 0.3}          \n",
      "{'loss': 2.0194, 'learning_rate': 4.695152183280265e-05, 'epoch': 0.3}          \n",
      "{'loss': 2.0464, 'learning_rate': 4.692359778174472e-05, 'epoch': 0.31}         \n",
      "{'loss': 2.09, 'learning_rate': 4.689572969070896e-05, 'epoch': 0.31}           \n",
      "{'loss': 2.076, 'learning_rate': 4.686780563965104e-05, 'epoch': 0.31}          \n",
      "{'loss': 2.0396, 'learning_rate': 4.6839825628570946e-05, 'epoch': 0.32}        \n",
      "{'loss': 2.0473, 'learning_rate': 4.681184561749087e-05, 'epoch': 0.32}         \n",
      "{'loss': 2.0087, 'learning_rate': 4.6783865606410784e-05, 'epoch': 0.32}        \n",
      "{'loss': 2.0588, 'learning_rate': 4.67558855953307e-05, 'epoch': 0.32}          \n",
      "{'loss': 2.0288, 'learning_rate': 4.6727905584250615e-05, 'epoch': 0.33}        \n",
      "{'loss': 2.0408, 'learning_rate': 4.669992557317053e-05, 'epoch': 0.33}         \n",
      "{'loss': 2.053, 'learning_rate': 4.6671945562090446e-05, 'epoch': 0.33}         \n",
      "{'loss': 2.026, 'learning_rate': 4.664396555101036e-05, 'epoch': 0.34}          \n",
      "{'loss': 2.0401, 'learning_rate': 4.661598553993027e-05, 'epoch': 0.34}         \n",
      "{'loss': 2.0708, 'learning_rate': 4.658800552885019e-05, 'epoch': 0.34}         \n",
      "{'loss': 1.989, 'learning_rate': 4.656002551777011e-05, 'epoch': 0.34}          \n",
      "{'loss': 1.9984, 'learning_rate': 4.653210146671218e-05, 'epoch': 0.35}         \n",
      "{'loss': 2.0172, 'learning_rate': 4.65041214556321e-05, 'epoch': 0.35}          \n",
      "{'loss': 2.0509, 'learning_rate': 4.6476141444552014e-05, 'epoch': 0.35}        \n",
      "{'loss': 2.0412, 'learning_rate': 4.644816143347193e-05, 'epoch': 0.36}         \n",
      "{'loss': 2.0569, 'learning_rate': 4.6420181422391845e-05, 'epoch': 0.36}        \n",
      "{'loss': 2.0319, 'learning_rate': 4.639220141131176e-05, 'epoch': 0.36}         \n",
      "{'loss': 1.9988, 'learning_rate': 4.636422140023168e-05, 'epoch': 0.36}         \n",
      "{'loss': 2.0272, 'learning_rate': 4.633624138915159e-05, 'epoch': 0.37}         \n",
      "{'loss': 2.0312, 'learning_rate': 4.630826137807151e-05, 'epoch': 0.37}         \n",
      "{'loss': 2.0361, 'learning_rate': 4.628033732701358e-05, 'epoch': 0.37}         \n",
      "{'loss': 2.0065, 'learning_rate': 4.62523573159335e-05, 'epoch': 0.37}          \n",
      "{'loss': 2.003, 'learning_rate': 4.6224377304853413e-05, 'epoch': 0.38}         \n",
      "{'loss': 2.0207, 'learning_rate': 4.619639729377333e-05, 'epoch': 0.38}         \n",
      "{'loss': 1.9854, 'learning_rate': 4.6168473242715404e-05, 'epoch': 0.38}        \n",
      "{'loss': 2.0855, 'learning_rate': 4.6140493231635326e-05, 'epoch': 0.39}        \n",
      "{'loss': 1.9946, 'learning_rate': 4.6112513220555235e-05, 'epoch': 0.39}        \n",
      "{'loss': 2.0215, 'learning_rate': 4.608453320947516e-05, 'epoch': 0.39}         \n",
      "{'loss': 1.9937, 'learning_rate': 4.605655319839507e-05, 'epoch': 0.39}         \n",
      "{'loss': 2.0399, 'learning_rate': 4.602857318731498e-05, 'epoch': 0.4}          \n",
      "{'loss': 2.0335, 'learning_rate': 4.6000593176234904e-05, 'epoch': 0.4}         \n",
      "{'loss': 2.0524, 'learning_rate': 4.597261316515481e-05, 'epoch': 0.4}          \n",
      "{'loss': 2.0063, 'learning_rate': 4.594474507411905e-05, 'epoch': 0.41}         \n",
      "{'loss': 2.0388, 'learning_rate': 4.591676506303897e-05, 'epoch': 0.41}         \n",
      "{'loss': 2.0047, 'learning_rate': 4.5888785051958884e-05, 'epoch': 0.41}        \n",
      "{'loss': 2.0228, 'learning_rate': 4.586080504087879e-05, 'epoch': 0.41}         \n",
      "{'loss': 1.9939, 'learning_rate': 4.5832825029798715e-05, 'epoch': 0.42}        \n",
      "{'loss': 2.0271, 'learning_rate': 4.580484501871863e-05, 'epoch': 0.42}         \n",
      "{'loss': 2.0295, 'learning_rate': 4.5776865007638546e-05, 'epoch': 0.42}        \n",
      "{'loss': 1.9836, 'learning_rate': 4.574888499655846e-05, 'epoch': 0.43}         \n",
      "{'loss': 1.9636, 'learning_rate': 4.572090498547838e-05, 'epoch': 0.43}         \n",
      "{'loss': 2.0256, 'learning_rate': 4.569292497439829e-05, 'epoch': 0.43}         \n",
      "{'loss': 1.9789, 'learning_rate': 4.566494496331821e-05, 'epoch': 0.43}         \n",
      "{'loss': 2.0003, 'learning_rate': 4.5636964952238124e-05, 'epoch': 0.44}        \n",
      "{'loss': 1.9684, 'learning_rate': 4.56090409011802e-05, 'epoch': 0.44}          \n",
      "{'loss': 1.9909, 'learning_rate': 4.5581060890100114e-05, 'epoch': 0.44}        \n",
      "{'loss': 2.0198, 'learning_rate': 4.555308087902003e-05, 'epoch': 0.44}         \n",
      "{'loss': 2.0081, 'learning_rate': 4.5525156827962104e-05, 'epoch': 0.45}        \n",
      "{'loss': 1.9764, 'learning_rate': 4.549717681688202e-05, 'epoch': 0.45}         \n",
      "{'loss': 1.989, 'learning_rate': 4.546919680580194e-05, 'epoch': 0.45}          \n",
      "{'loss': 2.0383, 'learning_rate': 4.544121679472185e-05, 'epoch': 0.46}         \n",
      "{'loss': 2.0217, 'learning_rate': 4.5413292743663925e-05, 'epoch': 0.46}        \n",
      "{'loss': 2.0077, 'learning_rate': 4.538531273258385e-05, 'epoch': 0.46}         \n",
      "{'loss': 1.9972, 'learning_rate': 4.5357332721503757e-05, 'epoch': 0.46}        \n",
      "{'loss': 2.0151, 'learning_rate': 4.532935271042368e-05, 'epoch': 0.47}         \n",
      "{'loss': 2.0632, 'learning_rate': 4.5301372699343594e-05, 'epoch': 0.47}        \n",
      "{'loss': 1.9981, 'learning_rate': 4.52733926882635e-05, 'epoch': 0.47}          \n",
      "{'loss': 1.9737, 'learning_rate': 4.5245468637205585e-05, 'epoch': 0.48}        \n",
      "{'loss': 2.0094, 'learning_rate': 4.52174886261255e-05, 'epoch': 0.48}          \n",
      "{'loss': 1.9615, 'learning_rate': 4.518950861504541e-05, 'epoch': 0.48}         \n",
      "{'loss': 2.0162, 'learning_rate': 4.516152860396533e-05, 'epoch': 0.48}         \n",
      "{'loss': 1.9687, 'learning_rate': 4.513354859288525e-05, 'epoch': 0.49}         \n",
      "{'loss': 1.9992, 'learning_rate': 4.510562454182732e-05, 'epoch': 0.49}         \n",
      "{'loss': 1.9659, 'learning_rate': 4.507764453074724e-05, 'epoch': 0.49}         \n",
      "{'loss': 2.0226, 'learning_rate': 4.504966451966715e-05, 'epoch': 0.5}          \n",
      "{'loss': 1.9935, 'learning_rate': 4.502168450858707e-05, 'epoch': 0.5}          \n",
      "{'loss': 1.9938, 'learning_rate': 4.4993704497506984e-05, 'epoch': 0.5}         \n",
      "{'loss': 1.9229, 'learning_rate': 4.49657244864269e-05, 'epoch': 0.5}           \n",
      "{'loss': 1.9945, 'learning_rate': 4.4937744475346815e-05, 'epoch': 0.51}        \n",
      "{'loss': 2.0024, 'learning_rate': 4.490976446426673e-05, 'epoch': 0.51}         \n",
      "{'loss': 1.9442, 'learning_rate': 4.4881784453186646e-05, 'epoch': 0.51}        \n",
      "{'loss': 2.0054, 'learning_rate': 4.485386040212872e-05, 'epoch': 0.51}         \n",
      "{'loss': 1.9438, 'learning_rate': 4.4825880391048636e-05, 'epoch': 0.52}        \n",
      "{'loss': 1.9363, 'learning_rate': 4.479790037996855e-05, 'epoch': 0.52}         \n",
      "{'loss': 2.0093, 'learning_rate': 4.476992036888847e-05, 'epoch': 0.52}         \n",
      "{'loss': 2.0173, 'learning_rate': 4.474194035780838e-05, 'epoch': 0.53}         \n",
      "{'loss': 1.946, 'learning_rate': 4.4714016306750464e-05, 'epoch': 0.53}         \n",
      "{'loss': 1.9883, 'learning_rate': 4.468603629567037e-05, 'epoch': 0.53}         \n",
      "{'loss': 1.9527, 'learning_rate': 4.465805628459029e-05, 'epoch': 0.53}         \n",
      "{'loss': 2.0017, 'learning_rate': 4.4630076273510204e-05, 'epoch': 0.54}        \n",
      "{'loss': 2.0136, 'learning_rate': 4.460209626243012e-05, 'epoch': 0.54}         \n",
      "{'loss': 1.9719, 'learning_rate': 4.45741722113722e-05, 'epoch': 0.54}          \n",
      "{'loss': 1.9722, 'learning_rate': 4.4546192200292116e-05, 'epoch': 0.55}        \n",
      "{'loss': 1.9751, 'learning_rate': 4.4518212189212025e-05, 'epoch': 0.55}        \n",
      "{'loss': 1.9658, 'learning_rate': 4.449023217813195e-05, 'epoch': 0.55}         \n",
      "{'loss': 1.941, 'learning_rate': 4.446225216705186e-05, 'epoch': 0.55}          \n",
      "{'loss': 1.9746, 'learning_rate': 4.443432811599393e-05, 'epoch': 0.56}         \n",
      "{'loss': 1.963, 'learning_rate': 4.440640406493601e-05, 'epoch': 0.56}          \n",
      "{'loss': 2.0117, 'learning_rate': 4.437842405385593e-05, 'epoch': 0.56}         \n",
      "{'loss': 1.9681, 'learning_rate': 4.4350500002798e-05, 'epoch': 0.57}           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0259, 'learning_rate': 4.432251999171792e-05, 'epoch': 0.57}         \n",
      "{'loss': 1.9915, 'learning_rate': 4.429453998063783e-05, 'epoch': 0.57}         \n",
      "{'loss': 1.9834, 'learning_rate': 4.4266559969557756e-05, 'epoch': 0.57}        \n",
      "{'loss': 1.9805, 'learning_rate': 4.4238579958477665e-05, 'epoch': 0.58}        \n",
      "{'loss': 1.958, 'learning_rate': 4.421059994739758e-05, 'epoch': 0.58}          \n",
      "{'loss': 1.9347, 'learning_rate': 4.418267589633966e-05, 'epoch': 0.58}         \n",
      "{'loss': 1.9418, 'learning_rate': 4.415469588525957e-05, 'epoch': 0.58}         \n",
      "{'loss': 1.9725, 'learning_rate': 4.4126715874179486e-05, 'epoch': 0.59}        \n",
      "{'loss': 1.9488, 'learning_rate': 4.409873586309941e-05, 'epoch': 0.59}         \n",
      "{'loss': 1.95, 'learning_rate': 4.407075585201932e-05, 'epoch': 0.59}           \n",
      "{'loss': 1.967, 'learning_rate': 4.404277584093924e-05, 'epoch': 0.6}           \n",
      "{'loss': 1.9684, 'learning_rate': 4.401479582985915e-05, 'epoch': 0.6}          \n",
      "{'loss': 1.9799, 'learning_rate': 4.3986815818779064e-05, 'epoch': 0.6}         \n",
      "{'loss': 1.9705, 'learning_rate': 4.3958835807698986e-05, 'epoch': 0.6}         \n",
      "{'loss': 2.0597, 'learning_rate': 4.3930855796618895e-05, 'epoch': 0.61}        \n",
      "{'loss': 1.9324, 'learning_rate': 4.390287578553881e-05, 'epoch': 0.61}         \n",
      "{'loss': 1.9755, 'learning_rate': 4.387489577445873e-05, 'epoch': 0.61}         \n",
      "{'loss': 2.0226, 'learning_rate': 4.38469717234008e-05, 'epoch': 0.62}          \n",
      "{'loss': 1.9508, 'learning_rate': 4.381899171232072e-05, 'epoch': 0.62}         \n",
      "{'loss': 2.0084, 'learning_rate': 4.37910676612628e-05, 'epoch': 0.62}          \n",
      "{'loss': 2.0008, 'learning_rate': 4.376308765018271e-05, 'epoch': 0.62}         \n",
      "{'loss': 1.9555, 'learning_rate': 4.373510763910263e-05, 'epoch': 0.63}         \n",
      "{'loss': 1.9889, 'learning_rate': 4.3707127628022544e-05, 'epoch': 0.63}        \n",
      "{'loss': 1.954, 'learning_rate': 4.367914761694246e-05, 'epoch': 0.63}          \n",
      "{'loss': 2.0043, 'learning_rate': 4.3651167605862375e-05, 'epoch': 0.64}        \n",
      "{'loss': 1.9603, 'learning_rate': 4.362318759478229e-05, 'epoch': 0.64}         \n",
      "{'loss': 1.9858, 'learning_rate': 4.3595207583702206e-05, 'epoch': 0.64}        \n",
      "{'loss': 1.9535, 'learning_rate': 4.356722757262212e-05, 'epoch': 0.64}         \n",
      "{'loss': 1.9843, 'learning_rate': 4.3539359481586355e-05, 'epoch': 0.65}        \n",
      "{'loss': 1.9552, 'learning_rate': 4.351137947050628e-05, 'epoch': 0.65}         \n",
      "{'loss': 1.9309, 'learning_rate': 4.3483399459426186e-05, 'epoch': 0.65}        \n",
      "{'loss': 1.9514, 'learning_rate': 4.34554194483461e-05, 'epoch': 0.65}          \n",
      "{'loss': 1.9575, 'learning_rate': 4.342743943726602e-05, 'epoch': 0.66}         \n",
      "{'loss': 1.9906, 'learning_rate': 4.339945942618593e-05, 'epoch': 0.66}         \n",
      "{'loss': 1.986, 'learning_rate': 4.337147941510585e-05, 'epoch': 0.66}          \n",
      "{'loss': 2.0003, 'learning_rate': 4.3343499404025764e-05, 'epoch': 0.67}        \n",
      "{'loss': 1.9646, 'learning_rate': 4.331551939294568e-05, 'epoch': 0.67}         \n",
      "{'loss': 1.9581, 'learning_rate': 4.328765130190991e-05, 'epoch': 0.67}         \n",
      "{'loss': 1.9571, 'learning_rate': 4.3259671290829836e-05, 'epoch': 0.67}        \n",
      "{'loss': 1.9445, 'learning_rate': 4.3231691279749744e-05, 'epoch': 0.68}        \n",
      "{'loss': 1.9677, 'learning_rate': 4.320371126866967e-05, 'epoch': 0.68}         \n",
      "{'loss': 1.9414, 'learning_rate': 4.317573125758958e-05, 'epoch': 0.68}         \n",
      "{'loss': 1.9235, 'learning_rate': 4.314775124650949e-05, 'epoch': 0.69}         \n",
      "{'loss': 1.9392, 'learning_rate': 4.3119771235429414e-05, 'epoch': 0.69}        \n",
      "{'loss': 1.9858, 'learning_rate': 4.309179122434933e-05, 'epoch': 0.69}         \n",
      "{'loss': 1.9558, 'learning_rate': 4.3063811213269245e-05, 'epoch': 0.69}        \n",
      "{'loss': 1.9223, 'learning_rate': 4.303583120218916e-05, 'epoch': 0.7}          \n",
      "{'loss': 1.9412, 'learning_rate': 4.3007907151131235e-05, 'epoch': 0.7}         \n",
      "{'loss': 1.9564, 'learning_rate': 4.297992714005115e-05, 'epoch': 0.7}          \n",
      "{'loss': 1.9678, 'learning_rate': 4.2952003088993225e-05, 'epoch': 0.71}        \n",
      "{'loss': 1.9802, 'learning_rate': 4.292402307791314e-05, 'epoch': 0.71}         \n",
      "{'loss': 1.9481, 'learning_rate': 4.2896043066833056e-05, 'epoch': 0.71}        \n",
      "{'loss': 1.8984, 'learning_rate': 4.286806305575297e-05, 'epoch': 0.71}         \n",
      "{'loss': 1.9572, 'learning_rate': 4.284008304467289e-05, 'epoch': 0.72}         \n",
      "{'loss': 1.9432, 'learning_rate': 4.281215899361496e-05, 'epoch': 0.72}         \n",
      "{'loss': 1.9944, 'learning_rate': 4.278417898253488e-05, 'epoch': 0.72}         \n",
      "{'loss': 1.9715, 'learning_rate': 4.27561989714548e-05, 'epoch': 0.72}          \n",
      "{'loss': 1.944, 'learning_rate': 4.272821896037471e-05, 'epoch': 0.73}          \n",
      "{'loss': 1.99, 'learning_rate': 4.270029490931678e-05, 'epoch': 0.73}           \n",
      "{'loss': 2.0008, 'learning_rate': 4.2672314898236705e-05, 'epoch': 0.73}        \n",
      "{'loss': 1.9413, 'learning_rate': 4.2644334887156614e-05, 'epoch': 0.74}        \n",
      "{'loss': 1.958, 'learning_rate': 4.261635487607653e-05, 'epoch': 0.74}          \n",
      "{'loss': 1.9864, 'learning_rate': 4.258837486499645e-05, 'epoch': 0.74}         \n",
      "{'loss': 2.0006, 'learning_rate': 4.256039485391636e-05, 'epoch': 0.74}         \n",
      "{'loss': 1.9522, 'learning_rate': 4.253241484283628e-05, 'epoch': 0.75}         \n",
      "{'loss': 1.9108, 'learning_rate': 4.25044348317562e-05, 'epoch': 0.75}          \n",
      "{'loss': 1.9617, 'learning_rate': 4.247645482067611e-05, 'epoch': 0.75}         \n",
      "{'loss': 1.9617, 'learning_rate': 4.244858672964035e-05, 'epoch': 0.76}         \n",
      "{'loss': 1.9402, 'learning_rate': 4.242060671856026e-05, 'epoch': 0.76}         \n",
      "{'loss': 1.9459, 'learning_rate': 4.239262670748018e-05, 'epoch': 0.76}         \n",
      "{'loss': 1.9788, 'learning_rate': 4.2364646696400094e-05, 'epoch': 0.76}        \n",
      "{'loss': 1.9755, 'learning_rate': 4.233666668532001e-05, 'epoch': 0.77}         \n",
      "{'loss': 1.9757, 'learning_rate': 4.2308686674239926e-05, 'epoch': 0.77}        \n",
      "{'loss': 1.9446, 'learning_rate': 4.228070666315984e-05, 'epoch': 0.77}         \n",
      "{'loss': 1.946, 'learning_rate': 4.225272665207976e-05, 'epoch': 0.78}          \n",
      "{'loss': 1.9903, 'learning_rate': 4.222480260102183e-05, 'epoch': 0.78}         \n",
      "{'loss': 1.9563, 'learning_rate': 4.219682258994175e-05, 'epoch': 0.78}         \n",
      "{'loss': 1.9785, 'learning_rate': 4.216884257886166e-05, 'epoch': 0.78}         \n",
      "{'loss': 1.9716, 'learning_rate': 4.214086256778158e-05, 'epoch': 0.79}         \n",
      "{'loss': 1.9437, 'learning_rate': 4.2112882556701493e-05, 'epoch': 0.79}        \n",
      "{'loss': 1.9669, 'learning_rate': 4.208495850564357e-05, 'epoch': 0.79}         \n",
      "{'loss': 1.9708, 'learning_rate': 4.205697849456349e-05, 'epoch': 0.79}         \n",
      "{'loss': 1.9655, 'learning_rate': 4.20289984834834e-05, 'epoch': 0.8}           \n",
      "{'loss': 1.9192, 'learning_rate': 4.200101847240332e-05, 'epoch': 0.8}          \n",
      "{'loss': 1.9834, 'learning_rate': 4.197303846132323e-05, 'epoch': 0.8}          \n",
      "{'loss': 1.9666, 'learning_rate': 4.1945114410265305e-05, 'epoch': 0.81}        \n",
      "{'loss': 1.9544, 'learning_rate': 4.191713439918523e-05, 'epoch': 0.81}         \n",
      "{'loss': 1.9705, 'learning_rate': 4.188915438810514e-05, 'epoch': 0.81}         \n",
      "{'loss': 1.9383, 'learning_rate': 4.186117437702505e-05, 'epoch': 0.81}         \n",
      "{'loss': 1.9032, 'learning_rate': 4.183325032596713e-05, 'epoch': 0.82}         \n",
      "{'loss': 1.9069, 'learning_rate': 4.180527031488705e-05, 'epoch': 0.82}         \n",
      "{'loss': 1.9734, 'learning_rate': 4.177734626382912e-05, 'epoch': 0.82}         \n",
      "{'loss': 1.9323, 'learning_rate': 4.174936625274904e-05, 'epoch': 0.83}         \n",
      "{'loss': 1.9063, 'learning_rate': 4.1721386241668954e-05, 'epoch': 0.83}        \n",
      "{'loss': 1.9624, 'learning_rate': 4.169340623058887e-05, 'epoch': 0.83}         \n",
      "{'loss': 1.9259, 'learning_rate': 4.1665426219508785e-05, 'epoch': 0.83}        \n",
      "{'loss': 1.9592, 'learning_rate': 4.16374462084287e-05, 'epoch': 0.84}          \n",
      "{'loss': 1.9262, 'learning_rate': 4.1609466197348616e-05, 'epoch': 0.84}        \n",
      "{'loss': 1.9406, 'learning_rate': 4.158148618626853e-05, 'epoch': 0.84}         \n",
      "{'loss': 1.9181, 'learning_rate': 4.155350617518845e-05, 'epoch': 0.84}         \n",
      "{'loss': 1.9357, 'learning_rate': 4.152552616410836e-05, 'epoch': 0.85}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8897, 'learning_rate': 4.149760211305044e-05, 'epoch': 0.85}         \n",
      "{'loss': 1.9534, 'learning_rate': 4.146962210197036e-05, 'epoch': 0.85}         \n",
      "{'loss': 1.9585, 'learning_rate': 4.144169805091243e-05, 'epoch': 0.86}         \n",
      "{'loss': 1.9679, 'learning_rate': 4.141371803983234e-05, 'epoch': 0.86}         \n",
      "{'loss': 1.9298, 'learning_rate': 4.1385738028752266e-05, 'epoch': 0.86}        \n",
      "{'loss': 1.9483, 'learning_rate': 4.1357758017672174e-05, 'epoch': 0.86}        \n",
      "{'loss': 1.9196, 'learning_rate': 4.132977800659209e-05, 'epoch': 0.87}         \n",
      "{'loss': 1.9411, 'learning_rate': 4.130179799551201e-05, 'epoch': 0.87}         \n",
      "{'loss': 1.9084, 'learning_rate': 4.127381798443192e-05, 'epoch': 0.87}         \n",
      "{'loss': 1.9022, 'learning_rate': 4.1245837973351843e-05, 'epoch': 0.88}        \n",
      "{'loss': 1.9281, 'learning_rate': 4.121785796227175e-05, 'epoch': 0.88}         \n",
      "{'loss': 1.9801, 'learning_rate': 4.118993391121383e-05, 'epoch': 0.88}         \n",
      "{'loss': 1.8987, 'learning_rate': 4.116195390013375e-05, 'epoch': 0.88}         \n",
      "{'loss': 1.976, 'learning_rate': 4.1133973889053665e-05, 'epoch': 0.89}         \n",
      "{'loss': 1.9543, 'learning_rate': 4.1105993877973573e-05, 'epoch': 0.89}        \n",
      "{'loss': 1.9698, 'learning_rate': 4.1078013866893496e-05, 'epoch': 0.89}        \n",
      "{'loss': 1.9577, 'learning_rate': 4.105008981583557e-05, 'epoch': 0.9}          \n",
      "{'loss': 1.9444, 'learning_rate': 4.102210980475548e-05, 'epoch': 0.9}          \n",
      "{'loss': 1.9152, 'learning_rate': 4.099418575369756e-05, 'epoch': 0.9}          \n",
      "{'loss': 1.9417, 'learning_rate': 4.0966205742617476e-05, 'epoch': 0.9}         \n",
      "{'loss': 1.921, 'learning_rate': 4.093822573153739e-05, 'epoch': 0.91}          \n",
      "{'loss': 1.9451, 'learning_rate': 4.091024572045731e-05, 'epoch': 0.91}         \n",
      "{'loss': 1.9501, 'learning_rate': 4.088226570937722e-05, 'epoch': 0.91}         \n",
      "{'loss': 1.9245, 'learning_rate': 4.085428569829714e-05, 'epoch': 0.91}         \n",
      "{'loss': 1.9763, 'learning_rate': 4.0826305687217054e-05, 'epoch': 0.92}        \n",
      "{'loss': 1.9027, 'learning_rate': 4.079832567613697e-05, 'epoch': 0.92}         \n",
      "{'loss': 1.9613, 'learning_rate': 4.0770345665056885e-05, 'epoch': 0.92}        \n",
      "{'loss': 1.8733, 'learning_rate': 4.07423656539768e-05, 'epoch': 0.93}          \n",
      "{'loss': 1.9139, 'learning_rate': 4.071444160291888e-05, 'epoch': 0.93}         \n",
      "{'loss': 1.9443, 'learning_rate': 4.068646159183879e-05, 'epoch': 0.93}         \n",
      "{'loss': 1.9289, 'learning_rate': 4.0658481580758706e-05, 'epoch': 0.93}        \n",
      "{'loss': 1.9155, 'learning_rate': 4.063050156967862e-05, 'epoch': 0.94}         \n",
      "{'loss': 1.959, 'learning_rate': 4.060252155859854e-05, 'epoch': 0.94}          \n",
      "{'loss': 1.9441, 'learning_rate': 4.057465346756277e-05, 'epoch': 0.94}         \n",
      "{'loss': 1.9373, 'learning_rate': 4.054667345648269e-05, 'epoch': 0.95}         \n",
      "{'loss': 1.8957, 'learning_rate': 4.051869344540261e-05, 'epoch': 0.95}         \n",
      "{'loss': 1.9356, 'learning_rate': 4.049071343432252e-05, 'epoch': 0.95}         \n",
      "{'loss': 1.8973, 'learning_rate': 4.046273342324244e-05, 'epoch': 0.95}         \n",
      "{'loss': 1.9172, 'learning_rate': 4.043475341216235e-05, 'epoch': 0.96}         \n",
      "{'loss': 1.9241, 'learning_rate': 4.040677340108227e-05, 'epoch': 0.96}         \n",
      "{'loss': 1.9012, 'learning_rate': 4.0378793390002187e-05, 'epoch': 0.96}        \n",
      "{'loss': 1.9322, 'learning_rate': 4.0350813378922095e-05, 'epoch': 0.97}        \n",
      "{'loss': 1.9506, 'learning_rate': 4.032288932786418e-05, 'epoch': 0.97}         \n",
      "{'loss': 1.961, 'learning_rate': 4.029490931678409e-05, 'epoch': 0.97}          \n",
      "{'loss': 1.9163, 'learning_rate': 4.0266929305704e-05, 'epoch': 0.97}           \n",
      "{'loss': 1.9694, 'learning_rate': 4.023894929462392e-05, 'epoch': 0.98}         \n",
      "{'loss': 1.9663, 'learning_rate': 4.021096928354384e-05, 'epoch': 0.98}         \n",
      "{'loss': 1.9663, 'learning_rate': 4.0182989272463754e-05, 'epoch': 0.98}        \n",
      "{'loss': 1.9092, 'learning_rate': 4.015512118142799e-05, 'epoch': 0.98}         \n",
      "{'loss': 1.8743, 'learning_rate': 4.0127141170347903e-05, 'epoch': 0.99}        \n",
      "{'loss': 1.9382, 'learning_rate': 4.0099161159267826e-05, 'epoch': 0.99}        \n",
      "{'loss': 1.9384, 'learning_rate': 4.0071181148187735e-05, 'epoch': 0.99}        \n",
      "{'loss': 1.9256, 'learning_rate': 4.004320113710765e-05, 'epoch': 1.0}          \n",
      "{'loss': 1.9347, 'learning_rate': 4.0015221126027566e-05, 'epoch': 1.0}         \n",
      "{'loss': 1.918, 'learning_rate': 3.998729707496964e-05, 'epoch': 1.0}           \n",
      "{'loss': 1.8739, 'learning_rate': 3.995931706388956e-05, 'epoch': 1.0}          \n",
      "{'loss': 1.8514, 'learning_rate': 3.993133705280948e-05, 'epoch': 1.01}         \n",
      "{'loss': 1.911, 'learning_rate': 3.990335704172939e-05, 'epoch': 1.01}          \n",
      "{'loss': 1.8652, 'learning_rate': 3.987537703064931e-05, 'epoch': 1.01}         \n",
      "{'loss': 1.8781, 'learning_rate': 3.984739701956922e-05, 'epoch': 1.02}         \n",
      "{'loss': 1.8988, 'learning_rate': 3.9819417008489134e-05, 'epoch': 1.02}        \n",
      "{'loss': 1.8953, 'learning_rate': 3.9791436997409056e-05, 'epoch': 1.02}        \n",
      "{'loss': 1.9236, 'learning_rate': 3.976351294635113e-05, 'epoch': 1.02}         \n",
      "{'loss': 1.8984, 'learning_rate': 3.9735532935271046e-05, 'epoch': 1.03}        \n",
      "{'loss': 1.8551, 'learning_rate': 3.970755292419096e-05, 'epoch': 1.03}         \n",
      "{'loss': 1.8704, 'learning_rate': 3.967957291311088e-05, 'epoch': 1.03}         \n",
      "{'loss': 1.8973, 'learning_rate': 3.965159290203079e-05, 'epoch': 1.04}         \n",
      "{'loss': 1.8818, 'learning_rate': 3.962361289095071e-05, 'epoch': 1.04}         \n",
      "{'loss': 1.8396, 'learning_rate': 3.959563287987062e-05, 'epoch': 1.04}         \n",
      "{'loss': 1.8593, 'learning_rate': 3.95677088288127e-05, 'epoch': 1.04}          \n",
      "{'loss': 1.8721, 'learning_rate': 3.953978477775477e-05, 'epoch': 1.05}         \n",
      "{'loss': 1.8912, 'learning_rate': 3.951180476667469e-05, 'epoch': 1.05}         \n",
      "{'loss': 1.9015, 'learning_rate': 3.9483824755594604e-05, 'epoch': 1.05}        \n",
      "{'loss': 1.9009, 'learning_rate': 3.945584474451452e-05, 'epoch': 1.05}         \n",
      "{'loss': 1.8825, 'learning_rate': 3.9427864733434435e-05, 'epoch': 1.06}        \n",
      "{'loss': 1.908, 'learning_rate': 3.939988472235435e-05, 'epoch': 1.06}          \n",
      "{'loss': 1.8815, 'learning_rate': 3.9371904711274266e-05, 'epoch': 1.06}        \n",
      "{'loss': 1.8962, 'learning_rate': 3.934392470019418e-05, 'epoch': 1.07}         \n",
      "{'loss': 1.8856, 'learning_rate': 3.93159446891141e-05, 'epoch': 1.07}          \n",
      "{'loss': 1.8846, 'learning_rate': 3.928796467803402e-05, 'epoch': 1.07}         \n",
      "{'loss': 1.8875, 'learning_rate': 3.9260040626976094e-05, 'epoch': 1.07}        \n",
      "{'loss': 1.9031, 'learning_rate': 3.9232060615896e-05, 'epoch': 1.08}           \n",
      "{'loss': 1.8643, 'learning_rate': 3.9204080604815926e-05, 'epoch': 1.08}        \n",
      "{'loss': 1.8935, 'learning_rate': 3.9176100593735834e-05, 'epoch': 1.08}        \n",
      "{'loss': 1.8804, 'learning_rate': 3.914812058265575e-05, 'epoch': 1.09}         \n",
      "{'loss': 1.8751, 'learning_rate': 3.912014057157567e-05, 'epoch': 1.09}         \n",
      "{'loss': 1.8828, 'learning_rate': 3.909216056049558e-05, 'epoch': 1.09}         \n",
      "{'loss': 1.8921, 'learning_rate': 3.9064236509437656e-05, 'epoch': 1.09}        \n",
      "{'loss': 1.8732, 'learning_rate': 3.903625649835758e-05, 'epoch': 1.1}          \n",
      "{'loss': 1.8696, 'learning_rate': 3.900827648727749e-05, 'epoch': 1.1}          \n",
      "{'loss': 1.8986, 'learning_rate': 3.898029647619741e-05, 'epoch': 1.1}          \n",
      "{'loss': 1.9096, 'learning_rate': 3.8952316465117325e-05, 'epoch': 1.11}        \n",
      "{'loss': 1.8748, 'learning_rate': 3.8924336454037234e-05, 'epoch': 1.11}        \n",
      "{'loss': 1.9029, 'learning_rate': 3.8896412402979315e-05, 'epoch': 1.11}        \n",
      "{'loss': 1.8867, 'learning_rate': 3.886848835192139e-05, 'epoch': 1.11}         \n",
      "{'loss': 1.8731, 'learning_rate': 3.8840508340841305e-05, 'epoch': 1.12}        \n",
      "{'loss': 1.8848, 'learning_rate': 3.881252832976122e-05, 'epoch': 1.12}         \n",
      "{'loss': 1.8449, 'learning_rate': 3.8784548318681136e-05, 'epoch': 1.12}        \n",
      "{'loss': 1.8588, 'learning_rate': 3.875656830760105e-05, 'epoch': 1.12}         \n",
      "{'loss': 1.9156, 'learning_rate': 3.872858829652097e-05, 'epoch': 1.13}         \n",
      "{'loss': 1.8932, 'learning_rate': 3.870060828544088e-05, 'epoch': 1.13}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8849, 'learning_rate': 3.86726282743608e-05, 'epoch': 1.13}          \n",
      "{'loss': 1.8671, 'learning_rate': 3.8644648263280714e-05, 'epoch': 1.14}        \n",
      "{'loss': 1.8909, 'learning_rate': 3.861666825220063e-05, 'epoch': 1.14}         \n",
      "{'loss': 1.9158, 'learning_rate': 3.8588744201142704e-05, 'epoch': 1.14}        \n",
      "{'loss': 1.9015, 'learning_rate': 3.856076419006262e-05, 'epoch': 1.14}         \n",
      "{'loss': 1.9012, 'learning_rate': 3.853278417898254e-05, 'epoch': 1.15}         \n",
      "{'loss': 1.8829, 'learning_rate': 3.850480416790245e-05, 'epoch': 1.15}         \n",
      "{'loss': 1.8671, 'learning_rate': 3.8476824156822366e-05, 'epoch': 1.15}        \n",
      "{'loss': 1.9072, 'learning_rate': 3.844884414574229e-05, 'epoch': 1.16}         \n",
      "{'loss': 1.8675, 'learning_rate': 3.8420920094684356e-05, 'epoch': 1.16}        \n",
      "{'loss': 1.8743, 'learning_rate': 3.839294008360427e-05, 'epoch': 1.16}         \n",
      "{'loss': 1.9224, 'learning_rate': 3.8364960072524194e-05, 'epoch': 1.16}        \n",
      "{'loss': 1.8922, 'learning_rate': 3.83369800614441e-05, 'epoch': 1.17}          \n",
      "{'loss': 1.8476, 'learning_rate': 3.8309000050364025e-05, 'epoch': 1.17}        \n",
      "{'loss': 1.8599, 'learning_rate': 3.828102003928394e-05, 'epoch': 1.17}         \n",
      "{'loss': 1.8859, 'learning_rate': 3.825309598822601e-05, 'epoch': 1.18}         \n",
      "{'loss': 1.8706, 'learning_rate': 3.822511597714593e-05, 'epoch': 1.18}         \n",
      "{'loss': 1.8788, 'learning_rate': 3.819713596606585e-05, 'epoch': 1.18}         \n",
      "{'loss': 1.8815, 'learning_rate': 3.8169155954985755e-05, 'epoch': 1.18}        \n",
      "{'loss': 1.8729, 'learning_rate': 3.814117594390568e-05, 'epoch': 1.19}         \n",
      "{'loss': 1.851, 'learning_rate': 3.811319593282559e-05, 'epoch': 1.19}          \n",
      "{'loss': 1.9027, 'learning_rate': 3.808527188176767e-05, 'epoch': 1.19}         \n",
      "{'loss': 1.8856, 'learning_rate': 3.8057291870687583e-05, 'epoch': 1.19}        \n",
      "{'loss': 1.8885, 'learning_rate': 3.80293118596075e-05, 'epoch': 1.2}           \n",
      "{'loss': 1.9161, 'learning_rate': 3.8001331848527415e-05, 'epoch': 1.2}         \n",
      "{'loss': 1.8715, 'learning_rate': 3.797335183744733e-05, 'epoch': 1.2}          \n",
      "{'loss': 1.8784, 'learning_rate': 3.7945427786389405e-05, 'epoch': 1.21}        \n",
      "{'loss': 1.8706, 'learning_rate': 3.791744777530932e-05, 'epoch': 1.21}         \n",
      "{'loss': 1.8886, 'learning_rate': 3.7889467764229236e-05, 'epoch': 1.21}        \n",
      "{'loss': 1.8867, 'learning_rate': 3.786148775314915e-05, 'epoch': 1.21}         \n",
      "{'loss': 1.8771, 'learning_rate': 3.7833563702091226e-05, 'epoch': 1.22}        \n",
      "{'loss': 1.9005, 'learning_rate': 3.78056396510333e-05, 'epoch': 1.22}          \n",
      "{'loss': 1.9105, 'learning_rate': 3.7777659639953216e-05, 'epoch': 1.22}        \n",
      "{'loss': 1.8703, 'learning_rate': 3.774967962887314e-05, 'epoch': 1.23}         \n",
      "{'loss': 1.8727, 'learning_rate': 3.772169961779305e-05, 'epoch': 1.23}         \n",
      "{'loss': 1.857, 'learning_rate': 3.769371960671297e-05, 'epoch': 1.23}          \n",
      "{'loss': 1.9066, 'learning_rate': 3.7665795555655044e-05, 'epoch': 1.23}        \n",
      "{'loss': 1.8846, 'learning_rate': 3.763781554457495e-05, 'epoch': 1.24}         \n",
      "{'loss': 1.8679, 'learning_rate': 3.7609835533494875e-05, 'epoch': 1.24}        \n",
      "{'loss': 1.8982, 'learning_rate': 3.758185552241479e-05, 'epoch': 1.24}         \n",
      "{'loss': 1.8726, 'learning_rate': 3.75538755113347e-05, 'epoch': 1.25}          \n",
      "{'loss': 1.8728, 'learning_rate': 3.752589550025462e-05, 'epoch': 1.25}         \n",
      "{'loss': 1.8435, 'learning_rate': 3.749791548917454e-05, 'epoch': 1.25}         \n",
      "{'loss': 1.8971, 'learning_rate': 3.746993547809445e-05, 'epoch': 1.25}         \n",
      "{'loss': 1.8725, 'learning_rate': 3.744195546701437e-05, 'epoch': 1.26}         \n",
      "{'loss': 1.8897, 'learning_rate': 3.741403141595644e-05, 'epoch': 1.26}         \n",
      "{'loss': 1.875, 'learning_rate': 3.738605140487636e-05, 'epoch': 1.26}          \n",
      "{'loss': 1.8681, 'learning_rate': 3.7358071393796274e-05, 'epoch': 1.26}        \n",
      "{'loss': 1.902, 'learning_rate': 3.733009138271619e-05, 'epoch': 1.27}          \n",
      "{'loss': 1.8561, 'learning_rate': 3.7302111371636105e-05, 'epoch': 1.27}        \n",
      "{'loss': 1.8971, 'learning_rate': 3.727418732057818e-05, 'epoch': 1.27}         \n",
      "{'loss': 1.8848, 'learning_rate': 3.72462073094981e-05, 'epoch': 1.28}          \n",
      "{'loss': 1.8822, 'learning_rate': 3.721822729841801e-05, 'epoch': 1.28}         \n",
      "{'loss': 1.8722, 'learning_rate': 3.7190247287337927e-05, 'epoch': 1.28}        \n",
      "{'loss': 1.8481, 'learning_rate': 3.716226727625784e-05, 'epoch': 1.28}         \n",
      "{'loss': 1.8839, 'learning_rate': 3.713428726517776e-05, 'epoch': 1.29}         \n",
      "{'loss': 1.9108, 'learning_rate': 3.710636321411983e-05, 'epoch': 1.29}         \n",
      "{'loss': 1.9068, 'learning_rate': 3.7078383203039755e-05, 'epoch': 1.29}        \n",
      "{'loss': 1.8854, 'learning_rate': 3.705045915198182e-05, 'epoch': 1.3}          \n",
      "{'loss': 1.9237, 'learning_rate': 3.702247914090174e-05, 'epoch': 1.3}          \n",
      "{'loss': 1.9121, 'learning_rate': 3.699449912982166e-05, 'epoch': 1.3}          \n",
      "{'loss': 1.8575, 'learning_rate': 3.696651911874157e-05, 'epoch': 1.3}          \n",
      "{'loss': 1.9105, 'learning_rate': 3.693853910766149e-05, 'epoch': 1.31}         \n",
      "{'loss': 1.8654, 'learning_rate': 3.691055909658141e-05, 'epoch': 1.31}         \n",
      "{'loss': 1.9087, 'learning_rate': 3.6882579085501316e-05, 'epoch': 1.31}        \n",
      "{'loss': 1.8627, 'learning_rate': 3.685459907442124e-05, 'epoch': 1.32}         \n",
      "{'loss': 1.885, 'learning_rate': 3.682661906334115e-05, 'epoch': 1.32}          \n",
      "{'loss': 1.8524, 'learning_rate': 3.679863905226107e-05, 'epoch': 1.32}         \n",
      "{'loss': 1.8959, 'learning_rate': 3.6770659041180985e-05, 'epoch': 1.32}        \n",
      "{'loss': 1.8938, 'learning_rate': 3.6742679030100894e-05, 'epoch': 1.33}        \n",
      "{'loss': 1.8679, 'learning_rate': 3.6714754979042975e-05, 'epoch': 1.33}        \n",
      "{'loss': 1.8938, 'learning_rate': 3.668677496796289e-05, 'epoch': 1.33}         \n",
      "{'loss': 1.8473, 'learning_rate': 3.66587949568828e-05, 'epoch': 1.33}          \n",
      "{'loss': 1.9278, 'learning_rate': 3.663081494580272e-05, 'epoch': 1.34}         \n",
      "{'loss': 1.8666, 'learning_rate': 3.660283493472264e-05, 'epoch': 1.34}         \n",
      "{'loss': 1.8427, 'learning_rate': 3.657491088366471e-05, 'epoch': 1.34}         \n",
      "{'loss': 1.8593, 'learning_rate': 3.654693087258463e-05, 'epoch': 1.35}         \n",
      "{'loss': 1.9124, 'learning_rate': 3.651895086150454e-05, 'epoch': 1.35}         \n",
      "{'loss': 1.8232, 'learning_rate': 3.649097085042446e-05, 'epoch': 1.35}         \n",
      "{'loss': 1.8288, 'learning_rate': 3.6462990839344374e-05, 'epoch': 1.35}        \n",
      "{'loss': 1.8782, 'learning_rate': 3.643501082826429e-05, 'epoch': 1.36}         \n",
      "{'loss': 1.867, 'learning_rate': 3.6407030817184205e-05, 'epoch': 1.36}         \n",
      "{'loss': 1.8633, 'learning_rate': 3.637905080610412e-05, 'epoch': 1.36}         \n",
      "{'loss': 1.9169, 'learning_rate': 3.6351126755046195e-05, 'epoch': 1.37}        \n",
      "{'loss': 1.8762, 'learning_rate': 3.632314674396611e-05, 'epoch': 1.37}         \n",
      "{'loss': 1.8604, 'learning_rate': 3.6295166732886026e-05, 'epoch': 1.37}        \n",
      "{'loss': 1.8761, 'learning_rate': 3.626718672180595e-05, 'epoch': 1.37}         \n",
      "{'loss': 1.8882, 'learning_rate': 3.623920671072586e-05, 'epoch': 1.38}         \n",
      "{'loss': 1.9014, 'learning_rate': 3.621128265966793e-05, 'epoch': 1.38}         \n",
      "{'loss': 1.8857, 'learning_rate': 3.6183302648587854e-05, 'epoch': 1.38}        \n",
      "{'loss': 1.8981, 'learning_rate': 3.615532263750776e-05, 'epoch': 1.39}         \n",
      "{'loss': 1.8575, 'learning_rate': 3.612734262642768e-05, 'epoch': 1.39}         \n",
      "{'loss': 1.8827, 'learning_rate': 3.60993626153476e-05, 'epoch': 1.39}          \n",
      "{'loss': 1.8918, 'learning_rate': 3.607138260426751e-05, 'epoch': 1.39}         \n",
      "{'loss': 1.8829, 'learning_rate': 3.604345855320959e-05, 'epoch': 1.4}          \n",
      "{'loss': 1.8199, 'learning_rate': 3.601547854212951e-05, 'epoch': 1.4}          \n",
      "{'loss': 1.859, 'learning_rate': 3.5987498531049416e-05, 'epoch': 1.4}          \n",
      "{'loss': 1.8681, 'learning_rate': 3.595951851996934e-05, 'epoch': 1.4}          \n",
      "{'loss': 1.9002, 'learning_rate': 3.5931538508889253e-05, 'epoch': 1.41}        \n",
      "{'loss': 1.8708, 'learning_rate': 3.590355849780916e-05, 'epoch': 1.41}         \n",
      "{'loss': 1.8629, 'learning_rate': 3.5875578486729085e-05, 'epoch': 1.41}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.909, 'learning_rate': 3.5847598475649e-05, 'epoch': 1.42}            \n",
      "{'loss': 1.8807, 'learning_rate': 3.5819674424591075e-05, 'epoch': 1.42}        \n",
      "{'loss': 1.8699, 'learning_rate': 3.579175037353315e-05, 'epoch': 1.42}         \n",
      "{'loss': 1.8641, 'learning_rate': 3.5763770362453065e-05, 'epoch': 1.42}        \n",
      "{'loss': 1.8713, 'learning_rate': 3.5735846311395146e-05, 'epoch': 1.43}        \n",
      "{'loss': 1.868, 'learning_rate': 3.5707866300315055e-05, 'epoch': 1.43}         \n",
      "{'loss': 1.8942, 'learning_rate': 3.567988628923497e-05, 'epoch': 1.43}         \n",
      "{'loss': 1.8741, 'learning_rate': 3.565190627815489e-05, 'epoch': 1.44}         \n",
      "{'loss': 1.8492, 'learning_rate': 3.56239262670748e-05, 'epoch': 1.44}          \n",
      "{'loss': 1.8596, 'learning_rate': 3.559594625599472e-05, 'epoch': 1.44}         \n",
      "{'loss': 1.908, 'learning_rate': 3.556796624491463e-05, 'epoch': 1.44}          \n",
      "{'loss': 1.9047, 'learning_rate': 3.553998623383455e-05, 'epoch': 1.45}         \n",
      "{'loss': 1.8997, 'learning_rate': 3.551206218277663e-05, 'epoch': 1.45}         \n",
      "{'loss': 1.8743, 'learning_rate': 3.5484082171696545e-05, 'epoch': 1.45}        \n",
      "{'loss': 1.8739, 'learning_rate': 3.5456102160616454e-05, 'epoch': 1.45}        \n",
      "{'loss': 1.897, 'learning_rate': 3.5428122149536376e-05, 'epoch': 1.46}         \n",
      "{'loss': 1.8599, 'learning_rate': 3.5400142138456285e-05, 'epoch': 1.46}        \n",
      "{'loss': 1.8715, 'learning_rate': 3.53721621273762e-05, 'epoch': 1.46}          \n",
      "{'loss': 1.9117, 'learning_rate': 3.534418211629612e-05, 'epoch': 1.47}         \n",
      "{'loss': 1.8984, 'learning_rate': 3.531620210521603e-05, 'epoch': 1.47}         \n",
      "{'loss': 1.88, 'learning_rate': 3.5288222094135954e-05, 'epoch': 1.47}          \n",
      "{'loss': 1.8484, 'learning_rate': 3.526035400310019e-05, 'epoch': 1.47}         \n",
      "{'loss': 1.8503, 'learning_rate': 3.52323739920201e-05, 'epoch': 1.48}          \n",
      "{'loss': 1.8659, 'learning_rate': 3.520439398094002e-05, 'epoch': 1.48}         \n",
      "{'loss': 1.8302, 'learning_rate': 3.5176413969859934e-05, 'epoch': 1.48}        \n",
      "{'loss': 1.8903, 'learning_rate': 3.514848991880201e-05, 'epoch': 1.49}         \n",
      "{'loss': 1.8736, 'learning_rate': 3.5120509907721924e-05, 'epoch': 1.49}        \n",
      "{'loss': 1.8822, 'learning_rate': 3.509252989664184e-05, 'epoch': 1.49}         \n",
      "{'loss': 1.8315, 'learning_rate': 3.5064549885561756e-05, 'epoch': 1.49}        \n",
      "{'loss': 1.8717, 'learning_rate': 3.503656987448167e-05, 'epoch': 1.5}          \n",
      "{'loss': 1.8426, 'learning_rate': 3.500858986340159e-05, 'epoch': 1.5}          \n",
      "{'loss': 1.8725, 'learning_rate': 3.49806098523215e-05, 'epoch': 1.5}           \n",
      "{'loss': 1.8833, 'learning_rate': 3.495262984124142e-05, 'epoch': 1.51}         \n",
      "{'loss': 1.8845, 'learning_rate': 3.492470579018349e-05, 'epoch': 1.51}         \n",
      "{'loss': 1.8606, 'learning_rate': 3.4896725779103415e-05, 'epoch': 1.51}        \n",
      "{'loss': 1.8901, 'learning_rate': 3.4868745768023323e-05, 'epoch': 1.51}        \n",
      "{'loss': 1.8806, 'learning_rate': 3.484076575694324e-05, 'epoch': 1.52}         \n",
      "{'loss': 1.8874, 'learning_rate': 3.4812785745863155e-05, 'epoch': 1.52}        \n",
      "{'loss': 1.8149, 'learning_rate': 3.478480573478307e-05, 'epoch': 1.52}         \n",
      "{'loss': 1.8669, 'learning_rate': 3.475682572370299e-05, 'epoch': 1.52}         \n",
      "{'loss': 1.8873, 'learning_rate': 3.47288457126229e-05, 'epoch': 1.53}          \n",
      "{'loss': 1.8768, 'learning_rate': 3.4700921661564976e-05, 'epoch': 1.53}        \n",
      "{'loss': 1.8881, 'learning_rate': 3.46729416504849e-05, 'epoch': 1.53}          \n",
      "{'loss': 1.8058, 'learning_rate': 3.4644961639404814e-05, 'epoch': 1.54}        \n",
      "{'loss': 1.8812, 'learning_rate': 3.461698162832472e-05, 'epoch': 1.54}         \n",
      "{'loss': 1.8448, 'learning_rate': 3.4589001617244645e-05, 'epoch': 1.54}        \n",
      "{'loss': 1.8837, 'learning_rate': 3.456107756618672e-05, 'epoch': 1.54}         \n",
      "{'loss': 1.8775, 'learning_rate': 3.4533097555106635e-05, 'epoch': 1.55}        \n",
      "{'loss': 1.8717, 'learning_rate': 3.450517350404871e-05, 'epoch': 1.55}         \n",
      "{'loss': 1.9175, 'learning_rate': 3.4477193492968625e-05, 'epoch': 1.55}        \n",
      "{'loss': 1.8162, 'learning_rate': 3.444921348188854e-05, 'epoch': 1.56}         \n",
      "{'loss': 1.89, 'learning_rate': 3.4421233470808456e-05, 'epoch': 1.56}          \n",
      "{'loss': 1.8858, 'learning_rate': 3.439325345972837e-05, 'epoch': 1.56}         \n",
      "{'loss': 1.8373, 'learning_rate': 3.436527344864829e-05, 'epoch': 1.56}         \n",
      "{'loss': 1.8706, 'learning_rate': 3.43372934375682e-05, 'epoch': 1.57}          \n",
      "{'loss': 1.8691, 'learning_rate': 3.430931342648812e-05, 'epoch': 1.57}         \n",
      "{'loss': 1.8577, 'learning_rate': 3.428138937543019e-05, 'epoch': 1.57}         \n",
      "{'loss': 1.893, 'learning_rate': 3.425340936435011e-05, 'epoch': 1.58}          \n",
      "{'loss': 1.9158, 'learning_rate': 3.422548531329219e-05, 'epoch': 1.58}         \n",
      "{'loss': 1.8629, 'learning_rate': 3.41975053022121e-05, 'epoch': 1.58}          \n",
      "{'loss': 1.8964, 'learning_rate': 3.4169525291132014e-05, 'epoch': 1.58}        \n",
      "{'loss': 1.8382, 'learning_rate': 3.4141545280051937e-05, 'epoch': 1.59}        \n",
      "{'loss': 1.8696, 'learning_rate': 3.4113565268971845e-05, 'epoch': 1.59}        \n",
      "{'loss': 1.8935, 'learning_rate': 3.408558525789176e-05, 'epoch': 1.59}         \n",
      "{'loss': 1.8509, 'learning_rate': 3.405760524681168e-05, 'epoch': 1.59}         \n",
      "{'loss': 1.8305, 'learning_rate': 3.402962523573159e-05, 'epoch': 1.6}          \n",
      "{'loss': 1.8867, 'learning_rate': 3.4001645224651514e-05, 'epoch': 1.6}         \n",
      "{'loss': 1.8229, 'learning_rate': 3.397366521357142e-05, 'epoch': 1.6}          \n",
      "{'loss': 1.8412, 'learning_rate': 3.394568520249134e-05, 'epoch': 1.61}         \n",
      "{'loss': 1.8563, 'learning_rate': 3.391770519141126e-05, 'epoch': 1.61}         \n",
      "{'loss': 1.8693, 'learning_rate': 3.3889781140353336e-05, 'epoch': 1.61}        \n",
      "{'loss': 1.8656, 'learning_rate': 3.3861857089295403e-05, 'epoch': 1.61}        \n",
      "{'loss': 1.8799, 'learning_rate': 3.3833877078215326e-05, 'epoch': 1.62}        \n",
      "{'loss': 1.8521, 'learning_rate': 3.380589706713524e-05, 'epoch': 1.62}         \n",
      "{'loss': 1.8658, 'learning_rate': 3.377791705605516e-05, 'epoch': 1.62}         \n",
      "{'loss': 1.8875, 'learning_rate': 3.374993704497507e-05, 'epoch': 1.63}         \n",
      "{'loss': 1.8557, 'learning_rate': 3.372195703389499e-05, 'epoch': 1.63}         \n",
      "{'loss': 1.8779, 'learning_rate': 3.369403298283706e-05, 'epoch': 1.63}         \n",
      "{'loss': 1.8511, 'learning_rate': 3.366605297175698e-05, 'epoch': 1.63}         \n",
      "{'loss': 1.8599, 'learning_rate': 3.3638072960676894e-05, 'epoch': 1.64}        \n",
      "{'loss': 1.8812, 'learning_rate': 3.361009294959681e-05, 'epoch': 1.64}         \n",
      "{'loss': 1.8789, 'learning_rate': 3.3582112938516725e-05, 'epoch': 1.64}        \n",
      "{'loss': 1.8595, 'learning_rate': 3.35541888874588e-05, 'epoch': 1.65}          \n",
      "{'loss': 1.8602, 'learning_rate': 3.3526208876378715e-05, 'epoch': 1.65}        \n",
      "{'loss': 1.8528, 'learning_rate': 3.349822886529863e-05, 'epoch': 1.65}         \n",
      "{'loss': 1.8429, 'learning_rate': 3.347024885421855e-05, 'epoch': 1.65}         \n",
      "{'loss': 1.8577, 'learning_rate': 3.344226884313846e-05, 'epoch': 1.66}         \n",
      "{'loss': 1.8891, 'learning_rate': 3.341428883205838e-05, 'epoch': 1.66}         \n",
      "{'loss': 1.8642, 'learning_rate': 3.338636478100046e-05, 'epoch': 1.66}         \n",
      "{'loss': 1.8697, 'learning_rate': 3.335838476992037e-05, 'epoch': 1.66}         \n",
      "{'loss': 1.8207, 'learning_rate': 3.333040475884028e-05, 'epoch': 1.67}         \n",
      "{'loss': 1.8396, 'learning_rate': 3.3302424747760205e-05, 'epoch': 1.67}        \n",
      "{'loss': 1.8614, 'learning_rate': 3.3274444736680114e-05, 'epoch': 1.67}        \n",
      "{'loss': 1.8702, 'learning_rate': 3.3246464725600036e-05, 'epoch': 1.68}        \n",
      "{'loss': 1.8552, 'learning_rate': 3.3218484714519945e-05, 'epoch': 1.68}        \n",
      "{'loss': 1.8332, 'learning_rate': 3.319050470343986e-05, 'epoch': 1.68}         \n",
      "{'loss': 1.8668, 'learning_rate': 3.316258065238194e-05, 'epoch': 1.68}         \n",
      "{'loss': 1.8822, 'learning_rate': 3.3134656601324017e-05, 'epoch': 1.69}        \n",
      "{'loss': 1.8841, 'learning_rate': 3.310667659024393e-05, 'epoch': 1.69}         \n",
      "{'loss': 1.9085, 'learning_rate': 3.307869657916385e-05, 'epoch': 1.69}         \n",
      "{'loss': 1.8675, 'learning_rate': 3.305071656808376e-05, 'epoch': 1.7}          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.887, 'learning_rate': 3.302279251702584e-05, 'epoch': 1.7}           \n",
      "{'loss': 1.8654, 'learning_rate': 3.299481250594575e-05, 'epoch': 1.7}          \n",
      "{'loss': 1.8494, 'learning_rate': 3.296683249486567e-05, 'epoch': 1.7}          \n",
      "{'loss': 1.9038, 'learning_rate': 3.2938852483785584e-05, 'epoch': 1.71}        \n",
      "{'loss': 1.865, 'learning_rate': 3.29108724727055e-05, 'epoch': 1.71}           \n",
      "{'loss': 1.8503, 'learning_rate': 3.2882892461625416e-05, 'epoch': 1.71}        \n",
      "{'loss': 1.8634, 'learning_rate': 3.285491245054533e-05, 'epoch': 1.72}         \n",
      "{'loss': 1.8632, 'learning_rate': 3.282693243946525e-05, 'epoch': 1.72}         \n",
      "{'loss': 1.8919, 'learning_rate': 3.279900838840732e-05, 'epoch': 1.72}         \n",
      "{'loss': 1.8833, 'learning_rate': 3.277102837732724e-05, 'epoch': 1.72}         \n",
      "{'loss': 1.8711, 'learning_rate': 3.274304836624715e-05, 'epoch': 1.73}         \n",
      "{'loss': 1.8216, 'learning_rate': 3.2715068355167075e-05, 'epoch': 1.73}        \n",
      "{'loss': 1.8445, 'learning_rate': 3.2687088344086984e-05, 'epoch': 1.73}        \n",
      "{'loss': 1.8358, 'learning_rate': 3.265916429302906e-05, 'epoch': 1.73}         \n",
      "{'loss': 1.8657, 'learning_rate': 3.263118428194898e-05, 'epoch': 1.74}         \n",
      "{'loss': 1.8347, 'learning_rate': 3.260320427086889e-05, 'epoch': 1.74}         \n",
      "{'loss': 1.8675, 'learning_rate': 3.2575224259788805e-05, 'epoch': 1.74}        \n",
      "{'loss': 1.8573, 'learning_rate': 3.254724424870873e-05, 'epoch': 1.75}         \n",
      "{'loss': 1.8795, 'learning_rate': 3.2519264237628636e-05, 'epoch': 1.75}        \n",
      "{'loss': 1.8592, 'learning_rate': 3.249134018657072e-05, 'epoch': 1.75}         \n",
      "{'loss': 1.8607, 'learning_rate': 3.246336017549063e-05, 'epoch': 1.75}         \n",
      "{'loss': 1.8723, 'learning_rate': 3.243538016441054e-05, 'epoch': 1.76}         \n",
      "{'loss': 1.8364, 'learning_rate': 3.2407400153330464e-05, 'epoch': 1.76}        \n",
      "{'loss': 1.8875, 'learning_rate': 3.237947610227254e-05, 'epoch': 1.76}         \n",
      "{'loss': 1.8755, 'learning_rate': 3.2351496091192454e-05, 'epoch': 1.77}        \n",
      "{'loss': 1.8641, 'learning_rate': 3.232351608011237e-05, 'epoch': 1.77}         \n",
      "{'loss': 1.8741, 'learning_rate': 3.2295536069032285e-05, 'epoch': 1.77}        \n",
      "{'loss': 1.8665, 'learning_rate': 3.22675560579522e-05, 'epoch': 1.77}          \n",
      "{'loss': 1.8619, 'learning_rate': 3.2239576046872116e-05, 'epoch': 1.78}        \n",
      "{'loss': 1.8628, 'learning_rate': 3.221159603579203e-05, 'epoch': 1.78}         \n",
      "{'loss': 1.849, 'learning_rate': 3.218361602471195e-05, 'epoch': 1.78}          \n",
      "{'loss': 1.8687, 'learning_rate': 3.215563601363186e-05, 'epoch': 1.79}         \n",
      "{'loss': 1.8843, 'learning_rate': 3.212765600255178e-05, 'epoch': 1.79}         \n",
      "{'loss': 1.89, 'learning_rate': 3.209973195149385e-05, 'epoch': 1.79}           \n",
      "{'loss': 1.8922, 'learning_rate': 3.207175194041377e-05, 'epoch': 1.79}         \n",
      "{'loss': 1.8597, 'learning_rate': 3.204377192933369e-05, 'epoch': 1.8}          \n",
      "{'loss': 1.8639, 'learning_rate': 3.20157919182536e-05, 'epoch': 1.8}           \n",
      "{'loss': 1.8738, 'learning_rate': 3.1987811907173515e-05, 'epoch': 1.8}         \n",
      "{'loss': 1.839, 'learning_rate': 3.195983189609343e-05, 'epoch': 1.8}           \n",
      "{'loss': 1.8723, 'learning_rate': 3.1931851885013347e-05, 'epoch': 1.81}        \n",
      "{'loss': 1.86, 'learning_rate': 3.190387187393326e-05, 'epoch': 1.81}           \n",
      "{'loss': 1.8457, 'learning_rate': 3.18760037828975e-05, 'epoch': 1.81}          \n",
      "{'loss': 1.8591, 'learning_rate': 3.184802377181742e-05, 'epoch': 1.82}         \n",
      "{'loss': 1.8322, 'learning_rate': 3.182004376073733e-05, 'epoch': 1.82}         \n",
      "{'loss': 1.8324, 'learning_rate': 3.179206374965725e-05, 'epoch': 1.82}         \n",
      "{'loss': 1.8516, 'learning_rate': 3.176408373857716e-05, 'epoch': 1.82}         \n",
      "{'loss': 1.8688, 'learning_rate': 3.173615968751924e-05, 'epoch': 1.83}         \n",
      "{'loss': 1.8602, 'learning_rate': 3.1708179676439155e-05, 'epoch': 1.83}        \n",
      "{'loss': 1.8567, 'learning_rate': 3.168019966535907e-05, 'epoch': 1.83}         \n",
      "{'loss': 1.8269, 'learning_rate': 3.1652219654278986e-05, 'epoch': 1.84}        \n",
      "{'loss': 1.8663, 'learning_rate': 3.16242396431989e-05, 'epoch': 1.84}          \n",
      "{'loss': 1.864, 'learning_rate': 3.1596315592140976e-05, 'epoch': 1.84}         \n",
      "{'loss': 1.8338, 'learning_rate': 3.156833558106089e-05, 'epoch': 1.84}         \n",
      "{'loss': 1.8515, 'learning_rate': 3.154035556998081e-05, 'epoch': 1.85}         \n",
      "{'loss': 1.8877, 'learning_rate': 3.151237555890072e-05, 'epoch': 1.85}         \n",
      "{'loss': 1.8556, 'learning_rate': 3.148439554782064e-05, 'epoch': 1.85}         \n",
      "{'loss': 1.8662, 'learning_rate': 3.145647149676271e-05, 'epoch': 1.86}         \n",
      "{'loss': 1.8535, 'learning_rate': 3.1428491485682635e-05, 'epoch': 1.86}        \n",
      "{'loss': 1.8927, 'learning_rate': 3.14005674346247e-05, 'epoch': 1.86}          \n",
      "{'loss': 1.8364, 'learning_rate': 3.137258742354462e-05, 'epoch': 1.86}         \n",
      "{'loss': 1.8836, 'learning_rate': 3.134460741246454e-05, 'epoch': 1.87}         \n",
      "{'loss': 1.8642, 'learning_rate': 3.131662740138445e-05, 'epoch': 1.87}         \n",
      "{'loss': 1.8604, 'learning_rate': 3.1288647390304365e-05, 'epoch': 1.87}        \n",
      "{'loss': 1.8561, 'learning_rate': 3.126066737922429e-05, 'epoch': 1.87}         \n",
      "{'loss': 1.8539, 'learning_rate': 3.1232687368144196e-05, 'epoch': 1.88}        \n",
      "{'loss': 1.8796, 'learning_rate': 3.120470735706412e-05, 'epoch': 1.88}         \n",
      "{'loss': 1.8624, 'learning_rate': 3.117678330600619e-05, 'epoch': 1.88}         \n",
      "{'loss': 1.8767, 'learning_rate': 3.11488032949261e-05, 'epoch': 1.89}          \n",
      "{'loss': 1.8141, 'learning_rate': 3.1120823283846024e-05, 'epoch': 1.89}        \n",
      "{'loss': 1.8301, 'learning_rate': 3.109284327276594e-05, 'epoch': 1.89}         \n",
      "{'loss': 1.8778, 'learning_rate': 3.106491922170801e-05, 'epoch': 1.89}         \n",
      "{'loss': 1.8783, 'learning_rate': 3.103693921062793e-05, 'epoch': 1.9}          \n",
      "{'loss': 1.8397, 'learning_rate': 3.1008959199547845e-05, 'epoch': 1.9}         \n",
      "{'loss': 1.8911, 'learning_rate': 3.098097918846776e-05, 'epoch': 1.9}          \n",
      "{'loss': 1.8708, 'learning_rate': 3.095299917738768e-05, 'epoch': 1.91}         \n",
      "{'loss': 1.8583, 'learning_rate': 3.092501916630759e-05, 'epoch': 1.91}         \n",
      "{'loss': 1.8725, 'learning_rate': 3.089703915522751e-05, 'epoch': 1.91}         \n",
      "{'loss': 1.8444, 'learning_rate': 3.086905914414742e-05, 'epoch': 1.91}         \n",
      "{'loss': 1.8367, 'learning_rate': 3.084107913306733e-05, 'epoch': 1.92}         \n",
      "{'loss': 1.833, 'learning_rate': 3.0813099121987254e-05, 'epoch': 1.92}         \n",
      "{'loss': 1.8681, 'learning_rate': 3.078517507092933e-05, 'epoch': 1.92}         \n",
      "{'loss': 1.8943, 'learning_rate': 3.0757195059849245e-05, 'epoch': 1.93}        \n",
      "{'loss': 1.8279, 'learning_rate': 3.072921504876916e-05, 'epoch': 1.93}         \n",
      "{'loss': 1.8733, 'learning_rate': 3.0701235037689076e-05, 'epoch': 1.93}        \n",
      "{'loss': 1.8584, 'learning_rate': 3.067325502660899e-05, 'epoch': 1.93}         \n",
      "{'loss': 1.8957, 'learning_rate': 3.0645330975551066e-05, 'epoch': 1.94}        \n",
      "{'loss': 1.8285, 'learning_rate': 3.061735096447098e-05, 'epoch': 1.94}         \n",
      "{'loss': 1.8662, 'learning_rate': 3.05893709533909e-05, 'epoch': 1.94}          \n",
      "{'loss': 1.8662, 'learning_rate': 3.056144690233297e-05, 'epoch': 1.94}         \n",
      "{'loss': 1.8496, 'learning_rate': 3.0533466891252894e-05, 'epoch': 1.95}        \n",
      "{'loss': 1.842, 'learning_rate': 3.0505486880172806e-05, 'epoch': 1.95}         \n",
      "{'loss': 1.8662, 'learning_rate': 3.047750686909272e-05, 'epoch': 1.95}         \n",
      "{'loss': 1.8686, 'learning_rate': 3.044952685801264e-05, 'epoch': 1.96}         \n",
      "{'loss': 1.8518, 'learning_rate': 3.0421546846932553e-05, 'epoch': 1.96}        \n",
      "{'loss': 1.8971, 'learning_rate': 3.0393566835852465e-05, 'epoch': 1.96}        \n",
      "{'loss': 1.8274, 'learning_rate': 3.0365586824772384e-05, 'epoch': 1.96}        \n",
      "{'loss': 1.8782, 'learning_rate': 3.033766277371446e-05, 'epoch': 1.97}         \n",
      "{'loss': 1.8671, 'learning_rate': 3.0309682762634377e-05, 'epoch': 1.97}        \n",
      "{'loss': 1.8765, 'learning_rate': 3.0281758711576452e-05, 'epoch': 1.97}        \n",
      "{'loss': 1.8661, 'learning_rate': 3.0253778700496367e-05, 'epoch': 1.98}        \n",
      "{'loss': 1.8268, 'learning_rate': 3.0225798689416286e-05, 'epoch': 1.98}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8806, 'learning_rate': 3.01978186783362e-05, 'epoch': 1.98}          \n",
      "{'loss': 1.8361, 'learning_rate': 3.016983866725611e-05, 'epoch': 1.98}         \n",
      "{'loss': 1.8402, 'learning_rate': 3.014185865617603e-05, 'epoch': 1.99}         \n",
      "{'loss': 1.8586, 'learning_rate': 3.0113878645095945e-05, 'epoch': 1.99}        \n",
      "{'loss': 1.8315, 'learning_rate': 3.0085898634015864e-05, 'epoch': 1.99}        \n",
      "{'loss': 1.85, 'learning_rate': 3.005797458295794e-05, 'epoch': 1.99}           \n",
      "{'loss': 1.8496, 'learning_rate': 3.002999457187785e-05, 'epoch': 2.0}          \n",
      "{'loss': 1.8153, 'learning_rate': 3.000201456079777e-05, 'epoch': 2.0}          \n",
      "{'loss': 1.7763, 'learning_rate': 2.9974034549717682e-05, 'epoch': 2.0}         \n",
      "{'loss': 1.7932, 'learning_rate': 2.9946110498659757e-05, 'epoch': 2.01}        \n",
      "{'loss': 1.8066, 'learning_rate': 2.9918130487579676e-05, 'epoch': 2.01}        \n",
      "{'loss': 1.7724, 'learning_rate': 2.989015047649959e-05, 'epoch': 2.01}         \n",
      "{'loss': 1.7872, 'learning_rate': 2.9862170465419503e-05, 'epoch': 2.01}        \n",
      "{'loss': 1.7884, 'learning_rate': 2.9834190454339422e-05, 'epoch': 2.02}        \n",
      "{'loss': 1.7869, 'learning_rate': 2.9806266403281497e-05, 'epoch': 2.02}        \n",
      "{'loss': 1.8101, 'learning_rate': 2.9778286392201416e-05, 'epoch': 2.02}        \n",
      "{'loss': 1.809, 'learning_rate': 2.9750306381121328e-05, 'epoch': 2.03}         \n",
      "{'loss': 1.7817, 'learning_rate': 2.9722326370041243e-05, 'epoch': 2.03}        \n",
      "{'loss': 1.7689, 'learning_rate': 2.9694346358961162e-05, 'epoch': 2.03}        \n",
      "{'loss': 1.7902, 'learning_rate': 2.9666366347881075e-05, 'epoch': 2.03}        \n",
      "{'loss': 1.8272, 'learning_rate': 2.9638386336800987e-05, 'epoch': 2.04}        \n",
      "{'loss': 1.7685, 'learning_rate': 2.9610406325720906e-05, 'epoch': 2.04}        \n",
      "{'loss': 1.7487, 'learning_rate': 2.958242631464082e-05, 'epoch': 2.04}         \n",
      "{'loss': 1.788, 'learning_rate': 2.95545022635829e-05, 'epoch': 2.05}           \n",
      "{'loss': 1.8125, 'learning_rate': 2.9526522252502815e-05, 'epoch': 2.05}        \n",
      "{'loss': 1.7518, 'learning_rate': 2.9498542241422727e-05, 'epoch': 2.05}        \n",
      "{'loss': 1.7982, 'learning_rate': 2.9470562230342646e-05, 'epoch': 2.05}        \n",
      "{'loss': 1.7876, 'learning_rate': 2.944263817928472e-05, 'epoch': 2.06}         \n",
      "{'loss': 1.7636, 'learning_rate': 2.9414658168204633e-05, 'epoch': 2.06}        \n",
      "{'loss': 1.7857, 'learning_rate': 2.938667815712455e-05, 'epoch': 2.06}         \n",
      "{'loss': 1.7862, 'learning_rate': 2.9358698146044467e-05, 'epoch': 2.06}        \n",
      "{'loss': 1.7775, 'learning_rate': 2.9330718134964386e-05, 'epoch': 2.07}        \n",
      "{'loss': 1.793, 'learning_rate': 2.93027381238843e-05, 'epoch': 2.07}           \n",
      "{'loss': 1.7916, 'learning_rate': 2.9274758112804214e-05, 'epoch': 2.07}        \n",
      "{'loss': 1.8015, 'learning_rate': 2.9246778101724133e-05, 'epoch': 2.08}        \n",
      "{'loss': 1.7982, 'learning_rate': 2.9218798090644045e-05, 'epoch': 2.08}        \n",
      "{'loss': 1.7773, 'learning_rate': 2.919087403958612e-05, 'epoch': 2.08}         \n",
      "{'loss': 1.7463, 'learning_rate': 2.916289402850604e-05, 'epoch': 2.08}         \n",
      "{'loss': 1.8042, 'learning_rate': 2.913491401742595e-05, 'epoch': 2.09}         \n",
      "{'loss': 1.801, 'learning_rate': 2.910693400634587e-05, 'epoch': 2.09}          \n",
      "{'loss': 1.8129, 'learning_rate': 2.9079009955287944e-05, 'epoch': 2.09}        \n",
      "{'loss': 1.8074, 'learning_rate': 2.905102994420786e-05, 'epoch': 2.1}          \n",
      "{'loss': 1.8373, 'learning_rate': 2.902304993312778e-05, 'epoch': 2.1}          \n",
      "{'loss': 1.8044, 'learning_rate': 2.899506992204769e-05, 'epoch': 2.1}          \n",
      "{'loss': 1.816, 'learning_rate': 2.8967089910967603e-05, 'epoch': 2.1}          \n",
      "{'loss': 1.8197, 'learning_rate': 2.8939165859909684e-05, 'epoch': 2.11}        \n",
      "{'loss': 1.8085, 'learning_rate': 2.891124180885176e-05, 'epoch': 2.11}         \n",
      "{'loss': 1.7899, 'learning_rate': 2.888326179777167e-05, 'epoch': 2.11}         \n",
      "{'loss': 1.8225, 'learning_rate': 2.885528178669159e-05, 'epoch': 2.12}         \n",
      "{'loss': 1.8046, 'learning_rate': 2.8827301775611502e-05, 'epoch': 2.12}        \n",
      "{'loss': 1.8119, 'learning_rate': 2.879932176453142e-05, 'epoch': 2.12}         \n",
      "{'loss': 1.7999, 'learning_rate': 2.8771341753451337e-05, 'epoch': 2.12}        \n",
      "{'loss': 1.8478, 'learning_rate': 2.874336174237125e-05, 'epoch': 2.13}         \n",
      "{'loss': 1.7729, 'learning_rate': 2.8715381731291168e-05, 'epoch': 2.13}        \n",
      "{'loss': 1.7814, 'learning_rate': 2.8687401720211083e-05, 'epoch': 2.13}        \n",
      "{'loss': 1.8086, 'learning_rate': 2.8659421709130996e-05, 'epoch': 2.13}        \n",
      "{'loss': 1.7996, 'learning_rate': 2.8631441698050915e-05, 'epoch': 2.14}        \n",
      "{'loss': 1.7903, 'learning_rate': 2.860351764699299e-05, 'epoch': 2.14}         \n",
      "{'loss': 1.7926, 'learning_rate': 2.8575537635912908e-05, 'epoch': 2.14}        \n",
      "{'loss': 1.7941, 'learning_rate': 2.854755762483282e-05, 'epoch': 2.15}         \n",
      "{'loss': 1.801, 'learning_rate': 2.8519577613752736e-05, 'epoch': 2.15}         \n",
      "{'loss': 1.7948, 'learning_rate': 2.8491653562694814e-05, 'epoch': 2.15}        \n",
      "{'loss': 1.8052, 'learning_rate': 2.846367355161473e-05, 'epoch': 2.15}         \n",
      "{'loss': 1.8106, 'learning_rate': 2.843569354053464e-05, 'epoch': 2.16}         \n",
      "{'loss': 1.8046, 'learning_rate': 2.840771352945456e-05, 'epoch': 2.16}         \n",
      "{'loss': 1.7349, 'learning_rate': 2.8379789478396635e-05, 'epoch': 2.16}        \n",
      "{'loss': 1.8073, 'learning_rate': 2.8351809467316547e-05, 'epoch': 2.17}        \n",
      "{'loss': 1.7851, 'learning_rate': 2.8323829456236466e-05, 'epoch': 2.17}        \n",
      "{'loss': 1.7977, 'learning_rate': 2.829584944515638e-05, 'epoch': 2.17}         \n",
      "{'loss': 1.7779, 'learning_rate': 2.82678694340763e-05, 'epoch': 2.17}          \n",
      "{'loss': 1.7546, 'learning_rate': 2.8239945383018375e-05, 'epoch': 2.18}        \n",
      "{'loss': 1.7881, 'learning_rate': 2.8211965371938287e-05, 'epoch': 2.18}        \n",
      "{'loss': 1.7854, 'learning_rate': 2.8184041320880365e-05, 'epoch': 2.18}        \n",
      "{'loss': 1.8041, 'learning_rate': 2.815606130980028e-05, 'epoch': 2.19}         \n",
      "{'loss': 1.7702, 'learning_rate': 2.8128081298720193e-05, 'epoch': 2.19}        \n",
      "{'loss': 1.7481, 'learning_rate': 2.8100101287640112e-05, 'epoch': 2.19}        \n",
      "{'loss': 1.8357, 'learning_rate': 2.8072121276560027e-05, 'epoch': 2.19}        \n",
      "{'loss': 1.8149, 'learning_rate': 2.8044141265479946e-05, 'epoch': 2.2}         \n",
      "{'loss': 1.7697, 'learning_rate': 2.801616125439986e-05, 'epoch': 2.2}          \n",
      "{'loss': 1.7779, 'learning_rate': 2.798818124331977e-05, 'epoch': 2.2}          \n",
      "{'loss': 1.7735, 'learning_rate': 2.796020123223969e-05, 'epoch': 2.2}          \n",
      "{'loss': 1.8101, 'learning_rate': 2.7932277181181764e-05, 'epoch': 2.21}        \n",
      "{'loss': 1.7786, 'learning_rate': 2.790429717010168e-05, 'epoch': 2.21}         \n",
      "{'loss': 1.7881, 'learning_rate': 2.78763171590216e-05, 'epoch': 2.21}          \n",
      "{'loss': 1.7974, 'learning_rate': 2.784833714794151e-05, 'epoch': 2.22}         \n",
      "{'loss': 1.812, 'learning_rate': 2.782035713686143e-05, 'epoch': 2.22}          \n",
      "{'loss': 1.788, 'learning_rate': 2.7792377125781342e-05, 'epoch': 2.22}         \n",
      "{'loss': 1.8232, 'learning_rate': 2.7764453074723417e-05, 'epoch': 2.22}        \n",
      "{'loss': 1.7552, 'learning_rate': 2.7736473063643336e-05, 'epoch': 2.23}        \n",
      "{'loss': 1.7387, 'learning_rate': 2.770854901258541e-05, 'epoch': 2.23}         \n",
      "{'loss': 1.7611, 'learning_rate': 2.7680569001505326e-05, 'epoch': 2.23}        \n",
      "{'loss': 1.7839, 'learning_rate': 2.7652588990425245e-05, 'epoch': 2.24}        \n",
      "{'loss': 1.7922, 'learning_rate': 2.7624608979345157e-05, 'epoch': 2.24}        \n",
      "{'loss': 1.7816, 'learning_rate': 2.759662896826507e-05, 'epoch': 2.24}         \n",
      "{'loss': 1.8121, 'learning_rate': 2.7568648957184988e-05, 'epoch': 2.24}        \n",
      "{'loss': 1.7845, 'learning_rate': 2.7540668946104904e-05, 'epoch': 2.25}        \n",
      "{'loss': 1.8017, 'learning_rate': 2.751274489504698e-05, 'epoch': 2.25}         \n",
      "{'loss': 1.761, 'learning_rate': 2.7484764883966897e-05, 'epoch': 2.25}         \n",
      "{'loss': 1.7798, 'learning_rate': 2.745678487288681e-05, 'epoch': 2.26}         \n",
      "{'loss': 1.8172, 'learning_rate': 2.7428804861806728e-05, 'epoch': 2.26}        \n",
      "{'loss': 1.8385, 'learning_rate': 2.740082485072664e-05, 'epoch': 2.26}         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7556, 'learning_rate': 2.7372844839646556e-05, 'epoch': 2.26}        \n",
      "{'loss': 1.8197, 'learning_rate': 2.7344864828566475e-05, 'epoch': 2.27}        \n",
      "{'loss': 1.8044, 'learning_rate': 2.7316884817486387e-05, 'epoch': 2.27}        \n",
      "{'loss': 1.794, 'learning_rate': 2.7288904806406306e-05, 'epoch': 2.27}         \n",
      "{'loss': 1.8053, 'learning_rate': 2.726098075534838e-05, 'epoch': 2.27}         \n",
      "{'loss': 1.8062, 'learning_rate': 2.7233000744268293e-05, 'epoch': 2.28}        \n",
      "{'loss': 1.7876, 'learning_rate': 2.7205020733188215e-05, 'epoch': 2.28}        \n",
      "{'loss': 1.7855, 'learning_rate': 2.7177040722108127e-05, 'epoch': 2.28}        \n",
      "{'loss': 1.7631, 'learning_rate': 2.7149116671050202e-05, 'epoch': 2.29}        \n",
      "{'loss': 1.792, 'learning_rate': 2.712113665997012e-05, 'epoch': 2.29}          \n",
      "{'loss': 1.7866, 'learning_rate': 2.7093156648890033e-05, 'epoch': 2.29}        \n",
      "{'loss': 1.7976, 'learning_rate': 2.7065176637809952e-05, 'epoch': 2.29}        \n",
      "{'loss': 1.7804, 'learning_rate': 2.7037196626729867e-05, 'epoch': 2.3}         \n",
      "{'loss': 1.7646, 'learning_rate': 2.700921661564978e-05, 'epoch': 2.3}          \n",
      "{'loss': 1.7882, 'learning_rate': 2.69812366045697e-05, 'epoch': 2.3}           \n",
      "{'loss': 1.7952, 'learning_rate': 2.695325659348961e-05, 'epoch': 2.31}         \n",
      "{'loss': 1.7992, 'learning_rate': 2.6925332542431685e-05, 'epoch': 2.31}        \n",
      "{'loss': 1.8003, 'learning_rate': 2.6897408491373767e-05, 'epoch': 2.31}        \n",
      "{'loss': 1.7879, 'learning_rate': 2.686942848029368e-05, 'epoch': 2.31}         \n",
      "{'loss': 1.7602, 'learning_rate': 2.684144846921359e-05, 'epoch': 2.32}         \n",
      "{'loss': 1.7854, 'learning_rate': 2.681346845813351e-05, 'epoch': 2.32}         \n",
      "{'loss': 1.7789, 'learning_rate': 2.6785488447053425e-05, 'epoch': 2.32}        \n",
      "{'loss': 1.8088, 'learning_rate': 2.6757508435973344e-05, 'epoch': 2.33}        \n",
      "{'loss': 1.7817, 'learning_rate': 2.6729528424893257e-05, 'epoch': 2.33}        \n",
      "{'loss': 1.8357, 'learning_rate': 2.6701548413813172e-05, 'epoch': 2.33}        \n",
      "{'loss': 1.7971, 'learning_rate': 2.667362436275525e-05, 'epoch': 2.33}         \n",
      "{'loss': 1.8128, 'learning_rate': 2.6645644351675166e-05, 'epoch': 2.34}        \n",
      "{'loss': 1.806, 'learning_rate': 2.6617664340595078e-05, 'epoch': 2.34}         \n",
      "{'loss': 1.8114, 'learning_rate': 2.6589684329514997e-05, 'epoch': 2.34}        \n",
      "{'loss': 1.8052, 'learning_rate': 2.656176027845707e-05, 'epoch': 2.34}         \n",
      "{'loss': 1.8015, 'learning_rate': 2.6533836227399146e-05, 'epoch': 2.35}        \n",
      "{'loss': 1.8172, 'learning_rate': 2.6505856216319065e-05, 'epoch': 2.35}        \n",
      "{'loss': 1.824, 'learning_rate': 2.6477876205238977e-05, 'epoch': 2.35}         \n",
      "{'loss': 1.7763, 'learning_rate': 2.6449896194158896e-05, 'epoch': 2.36}        \n",
      "{'loss': 1.8002, 'learning_rate': 2.6421916183078808e-05, 'epoch': 2.36}        \n",
      "{'loss': 1.8267, 'learning_rate': 2.6393936171998724e-05, 'epoch': 2.36}        \n",
      "{'loss': 1.7892, 'learning_rate': 2.6365956160918643e-05, 'epoch': 2.36}        \n",
      "{'loss': 1.81, 'learning_rate': 2.6337976149838555e-05, 'epoch': 2.37}          \n",
      "{'loss': 1.817, 'learning_rate': 2.631005209878063e-05, 'epoch': 2.37}          \n",
      "{'loss': 1.7673, 'learning_rate': 2.628207208770055e-05, 'epoch': 2.37}         \n",
      "{'loss': 1.7981, 'learning_rate': 2.6254092076620464e-05, 'epoch': 2.38}        \n",
      "{'loss': 1.7737, 'learning_rate': 2.6226168025562542e-05, 'epoch': 2.38}        \n",
      "{'loss': 1.8175, 'learning_rate': 2.6198188014482454e-05, 'epoch': 2.38}        \n",
      "{'loss': 1.8019, 'learning_rate': 2.617020800340237e-05, 'epoch': 2.38}         \n",
      "{'loss': 1.806, 'learning_rate': 2.614222799232229e-05, 'epoch': 2.39}          \n",
      "{'loss': 1.7929, 'learning_rate': 2.61142479812422e-05, 'epoch': 2.39}          \n",
      "{'loss': 1.8022, 'learning_rate': 2.6086267970162116e-05, 'epoch': 2.39}        \n",
      "{'loss': 1.777, 'learning_rate': 2.6058287959082035e-05, 'epoch': 2.4}          \n",
      "{'loss': 1.7826, 'learning_rate': 2.6030307948001947e-05, 'epoch': 2.4}         \n",
      "{'loss': 1.7785, 'learning_rate': 2.6002327936921866e-05, 'epoch': 2.4}         \n",
      "{'loss': 1.8336, 'learning_rate': 2.597440388586394e-05, 'epoch': 2.4}          \n",
      "{'loss': 1.8188, 'learning_rate': 2.5946423874783853e-05, 'epoch': 2.41}        \n",
      "{'loss': 1.7511, 'learning_rate': 2.5918443863703772e-05, 'epoch': 2.41}        \n",
      "{'loss': 1.7518, 'learning_rate': 2.5890463852623688e-05, 'epoch': 2.41}        \n",
      "{'loss': 1.7856, 'learning_rate': 2.5862539801565762e-05, 'epoch': 2.41}        \n",
      "{'loss': 1.8049, 'learning_rate': 2.583455979048568e-05, 'epoch': 2.42}         \n",
      "{'loss': 1.7719, 'learning_rate': 2.5806579779405593e-05, 'epoch': 2.42}        \n",
      "{'loss': 1.7903, 'learning_rate': 2.5778599768325512e-05, 'epoch': 2.42}        \n",
      "{'loss': 1.8113, 'learning_rate': 2.5750675717267587e-05, 'epoch': 2.43}        \n",
      "{'loss': 1.7764, 'learning_rate': 2.57226957061875e-05, 'epoch': 2.43}          \n",
      "{'loss': 1.7753, 'learning_rate': 2.5694715695107418e-05, 'epoch': 2.43}        \n",
      "{'loss': 1.8123, 'learning_rate': 2.5666735684027333e-05, 'epoch': 2.43}        \n",
      "{'loss': 1.8121, 'learning_rate': 2.5638755672947246e-05, 'epoch': 2.44}        \n",
      "{'loss': 1.7574, 'learning_rate': 2.5610775661867165e-05, 'epoch': 2.44}        \n",
      "{'loss': 1.7933, 'learning_rate': 2.5582795650787077e-05, 'epoch': 2.44}        \n",
      "{'loss': 1.7927, 'learning_rate': 2.5554815639706996e-05, 'epoch': 2.45}        \n",
      "{'loss': 1.8014, 'learning_rate': 2.552683562862691e-05, 'epoch': 2.45}         \n",
      "{'loss': 1.8006, 'learning_rate': 2.5498911577568986e-05, 'epoch': 2.45}        \n",
      "{'loss': 1.7687, 'learning_rate': 2.5470931566488905e-05, 'epoch': 2.45}        \n",
      "{'loss': 1.795, 'learning_rate': 2.5442951555408817e-05, 'epoch': 2.46}         \n",
      "{'loss': 1.7987, 'learning_rate': 2.541497154432873e-05, 'epoch': 2.46}         \n",
      "{'loss': 1.7971, 'learning_rate': 2.5386991533248648e-05, 'epoch': 2.46}        \n",
      "{'loss': 1.7903, 'learning_rate': 2.5359011522168564e-05, 'epoch': 2.47}        \n",
      "{'loss': 1.8142, 'learning_rate': 2.5331087471110638e-05, 'epoch': 2.47}        \n",
      "{'loss': 1.7987, 'learning_rate': 2.5303107460030557e-05, 'epoch': 2.47}        \n",
      "{'loss': 1.8108, 'learning_rate': 2.527512744895047e-05, 'epoch': 2.47}         \n",
      "{'loss': 1.8041, 'learning_rate': 2.5247147437870388e-05, 'epoch': 2.48}        \n",
      "{'loss': 1.7923, 'learning_rate': 2.5219167426790304e-05, 'epoch': 2.48}        \n",
      "{'loss': 1.7768, 'learning_rate': 2.5191187415710216e-05, 'epoch': 2.48}        \n",
      "{'loss': 1.7673, 'learning_rate': 2.5163207404630135e-05, 'epoch': 2.48}        \n",
      "{'loss': 1.8406, 'learning_rate': 2.5135227393550047e-05, 'epoch': 2.49}        \n",
      "{'loss': 1.7447, 'learning_rate': 2.510730334249212e-05, 'epoch': 2.49}         \n",
      "{'loss': 1.7838, 'learning_rate': 2.5079379291434203e-05, 'epoch': 2.49}        \n",
      "{'loss': 1.7565, 'learning_rate': 2.5051455240376277e-05, 'epoch': 2.5}         \n",
      "{'loss': 1.776, 'learning_rate': 2.502347522929619e-05, 'epoch': 2.5}           \n",
      "{'loss': 1.7907, 'learning_rate': 2.499549521821611e-05, 'epoch': 2.5}          \n",
      "{'loss': 1.7959, 'learning_rate': 2.496751520713602e-05, 'epoch': 2.5}          \n",
      "{'loss': 1.7715, 'learning_rate': 2.493953519605594e-05, 'epoch': 2.51}         \n",
      "{'loss': 1.8206, 'learning_rate': 2.4911555184975855e-05, 'epoch': 2.51}        \n",
      "{'loss': 1.7852, 'learning_rate': 2.488357517389577e-05, 'epoch': 2.51}         \n",
      "{'loss': 1.7552, 'learning_rate': 2.4855595162815686e-05, 'epoch': 2.52}        \n",
      "{'loss': 1.7888, 'learning_rate': 2.4827615151735602e-05, 'epoch': 2.52}        \n",
      "{'loss': 1.7717, 'learning_rate': 2.4799691100677677e-05, 'epoch': 2.52}        \n",
      "{'loss': 1.7962, 'learning_rate': 2.4771711089597592e-05, 'epoch': 2.52}        \n",
      "{'loss': 1.7617, 'learning_rate': 2.4743731078517508e-05, 'epoch': 2.53}        \n",
      "{'loss': 1.7831, 'learning_rate': 2.4715751067437423e-05, 'epoch': 2.53}        \n",
      "{'loss': 1.779, 'learning_rate': 2.468777105635734e-05, 'epoch': 2.53}          \n",
      "{'loss': 1.801, 'learning_rate': 2.4659791045277254e-05, 'epoch': 2.53}         \n",
      "{'loss': 1.7895, 'learning_rate': 2.4631811034197173e-05, 'epoch': 2.54}        \n",
      "{'loss': 1.8098, 'learning_rate': 2.4603886983139245e-05, 'epoch': 2.54}        \n",
      "{'loss': 1.8023, 'learning_rate': 2.4575906972059163e-05, 'epoch': 2.54}        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7989, 'learning_rate': 2.454792696097908e-05, 'epoch': 2.55}         \n",
      "{'loss': 1.7585, 'learning_rate': 2.451994694989899e-05, 'epoch': 2.55}         \n",
      "{'loss': 1.7695, 'learning_rate': 2.4491966938818907e-05, 'epoch': 2.55}        \n",
      "{'loss': 1.8124, 'learning_rate': 2.4464042887760985e-05, 'epoch': 2.55}        \n",
      "{'loss': 1.7809, 'learning_rate': 2.44360628766809e-05, 'epoch': 2.56}          \n",
      "{'loss': 1.7734, 'learning_rate': 2.4408082865600816e-05, 'epoch': 2.56}        \n",
      "{'loss': 1.8082, 'learning_rate': 2.438010285452073e-05, 'epoch': 2.56}         \n",
      "{'loss': 1.7998, 'learning_rate': 2.4352122843440647e-05, 'epoch': 2.57}        \n",
      "{'loss': 1.7731, 'learning_rate': 2.4324198792382725e-05, 'epoch': 2.57}        \n",
      "{'loss': 1.7746, 'learning_rate': 2.4296218781302637e-05, 'epoch': 2.57}        \n",
      "{'loss': 1.8021, 'learning_rate': 2.4268238770222553e-05, 'epoch': 2.57}        \n",
      "{'loss': 1.8036, 'learning_rate': 2.424025875914247e-05, 'epoch': 2.58}         \n",
      "{'loss': 1.8367, 'learning_rate': 2.4212278748062387e-05, 'epoch': 2.58}        \n",
      "{'loss': 1.74, 'learning_rate': 2.418435469700446e-05, 'epoch': 2.58}           \n",
      "{'loss': 1.7605, 'learning_rate': 2.4156374685924377e-05, 'epoch': 2.59}        \n",
      "{'loss': 1.7951, 'learning_rate': 2.4128394674844293e-05, 'epoch': 2.59}        \n",
      "{'loss': 1.7889, 'learning_rate': 2.410041466376421e-05, 'epoch': 2.59}         \n",
      "{'loss': 1.8011, 'learning_rate': 2.4072434652684124e-05, 'epoch': 2.59}        \n",
      "{'loss': 1.7771, 'learning_rate': 2.40445106016262e-05, 'epoch': 2.6}           \n",
      "{'loss': 1.7823, 'learning_rate': 2.4016530590546117e-05, 'epoch': 2.6}         \n",
      "{'loss': 1.791, 'learning_rate': 2.398855057946603e-05, 'epoch': 2.6}           \n",
      "{'loss': 1.789, 'learning_rate': 2.3960570568385945e-05, 'epoch': 2.6}          \n",
      "{'loss': 1.772, 'learning_rate': 2.3932646517328023e-05, 'epoch': 2.61}         \n",
      "{'loss': 1.8, 'learning_rate': 2.390466650624794e-05, 'epoch': 2.61}            \n",
      "{'loss': 1.8024, 'learning_rate': 2.387668649516785e-05, 'epoch': 2.61}         \n",
      "{'loss': 1.7633, 'learning_rate': 2.384870648408777e-05, 'epoch': 2.62}         \n",
      "{'loss': 1.7595, 'learning_rate': 2.3820726473007685e-05, 'epoch': 2.62}        \n",
      "{'loss': 1.7845, 'learning_rate': 2.379280242194976e-05, 'epoch': 2.62}         \n",
      "{'loss': 1.7639, 'learning_rate': 2.3764822410869675e-05, 'epoch': 2.62}        \n",
      "{'loss': 1.79, 'learning_rate': 2.373684239978959e-05, 'epoch': 2.63}           \n",
      "{'loss': 1.7797, 'learning_rate': 2.3708862388709507e-05, 'epoch': 2.63}        \n",
      "{'loss': 1.7675, 'learning_rate': 2.3680882377629422e-05, 'epoch': 2.63}        \n",
      "{'loss': 1.7894, 'learning_rate': 2.3652958326571497e-05, 'epoch': 2.64}        \n",
      "{'loss': 1.7429, 'learning_rate': 2.3624978315491412e-05, 'epoch': 2.64}        \n",
      "{'loss': 1.7838, 'learning_rate': 2.359699830441133e-05, 'epoch': 2.64}         \n",
      "{'loss': 1.7947, 'learning_rate': 2.3569018293331247e-05, 'epoch': 2.64}        \n",
      "{'loss': 1.7784, 'learning_rate': 2.354103828225116e-05, 'epoch': 2.65}         \n",
      "{'loss': 1.7997, 'learning_rate': 2.3513114231193237e-05, 'epoch': 2.65}        \n",
      "{'loss': 1.7805, 'learning_rate': 2.3485134220113152e-05, 'epoch': 2.65}        \n",
      "{'loss': 1.8005, 'learning_rate': 2.3457154209033068e-05, 'epoch': 2.66}        \n",
      "{'loss': 1.7647, 'learning_rate': 2.3429174197952984e-05, 'epoch': 2.66}        \n",
      "{'loss': 1.7913, 'learning_rate': 2.34011941868729e-05, 'epoch': 2.66}          \n",
      "{'loss': 1.8265, 'learning_rate': 2.3373270135814977e-05, 'epoch': 2.66}        \n",
      "{'loss': 1.7699, 'learning_rate': 2.334529012473489e-05, 'epoch': 2.67}         \n",
      "{'loss': 1.7774, 'learning_rate': 2.3317310113654805e-05, 'epoch': 2.67}        \n",
      "{'loss': 1.8077, 'learning_rate': 2.328933010257472e-05, 'epoch': 2.67}         \n",
      "{'loss': 1.8189, 'learning_rate': 2.326135009149464e-05, 'epoch': 2.67}         \n",
      "{'loss': 1.7868, 'learning_rate': 2.323342604043671e-05, 'epoch': 2.68}         \n",
      "{'loss': 1.805, 'learning_rate': 2.320544602935663e-05, 'epoch': 2.68}          \n",
      "{'loss': 1.811, 'learning_rate': 2.3177466018276545e-05, 'epoch': 2.68}         \n",
      "{'loss': 1.7863, 'learning_rate': 2.314948600719646e-05, 'epoch': 2.69}         \n",
      "{'loss': 1.7941, 'learning_rate': 2.3121561956138535e-05, 'epoch': 2.69}        \n",
      "{'loss': 1.7263, 'learning_rate': 2.309358194505845e-05, 'epoch': 2.69}         \n",
      "{'loss': 1.7729, 'learning_rate': 2.3065601933978366e-05, 'epoch': 2.69}        \n",
      "{'loss': 1.8122, 'learning_rate': 2.3037621922898285e-05, 'epoch': 2.7}         \n",
      "{'loss': 1.7902, 'learning_rate': 2.3009697871840356e-05, 'epoch': 2.7}         \n",
      "{'loss': 1.7885, 'learning_rate': 2.2981773820782434e-05, 'epoch': 2.7}         \n",
      "{'loss': 1.7452, 'learning_rate': 2.295379380970235e-05, 'epoch': 2.71}         \n",
      "{'loss': 1.7751, 'learning_rate': 2.2925813798622265e-05, 'epoch': 2.71}        \n",
      "{'loss': 1.8431, 'learning_rate': 2.289783378754218e-05, 'epoch': 2.71}         \n",
      "{'loss': 1.7967, 'learning_rate': 2.2869853776462097e-05, 'epoch': 2.71}        \n",
      "{'loss': 1.8224, 'learning_rate': 2.2841929725404174e-05, 'epoch': 2.72}        \n",
      "{'loss': 1.8262, 'learning_rate': 2.2813949714324087e-05, 'epoch': 2.72}        \n",
      "{'loss': 1.7961, 'learning_rate': 2.2785969703244002e-05, 'epoch': 2.72}        \n",
      "{'loss': 1.7469, 'learning_rate': 2.275798969216392e-05, 'epoch': 2.73}         \n",
      "{'loss': 1.7747, 'learning_rate': 2.2730009681083837e-05, 'epoch': 2.73}        \n",
      " 55%|███████████████▊             | 487752/893495 [24:48:45<21:37:15,  5.21it/s]"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"../../../data/train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"../../../data/eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--block_size=12 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--output_dir=\"saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
