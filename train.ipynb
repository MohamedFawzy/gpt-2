{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new GPT-2 model.\n",
    "- based on LABR dataset book reviews .\n",
    "- later on use this pre-trained model to generate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, paths):\n",
    "        trainer = BpeTrainer(vocab_size=5_00_00, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "        self.tokenizer.train(trainer, paths)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the folder 'text' contains all the files\n",
    "paths = [str(x) for x in Path(\"./data/\").glob(\"**/negative_labr.txt\")]\n",
    "tokenizer = BPE_token()\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(paths)\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/negative_labr.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenizer from the saved model path\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    "  \n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "for filename in paths:\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        x = f.read()\n",
    "    single_string += x + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "    examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "    inputs.append(ex[:-1])\n",
    "    labels.append(ex[1:])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  7 18:17:34 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 207...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   56C    P0    27W /  N/A |   7704MiB /  7973MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1440      G   /usr/lib/xorg/Xorg                 59MiB |\r\n",
      "|    0   N/A  N/A      3199      G   /usr/lib/xorg/Xorg                164MiB |\r\n",
      "|    0   N/A  N/A      3333      G   /usr/bin/gnome-shell               72MiB |\r\n",
      "|    0   N/A  N/A      4632      G   ...AAAAAAAAA= --shared-files      154MiB |\r\n",
      "|    0   N/A  N/A      8980      C   ...3/envs/pytorch/bin/python     7237MiB |\r\n",
      "|    0   N/A  N/A      9963      G   /usr/bin/nvidia-settings            0MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "494/494 [==============================] - 173s 350ms/step - loss: 8.6571 - logits_loss: 8.6571 - logits_accuracy: 0.0461 - past_key_values_1_accuracy: 6.5375e-04 - past_key_values_2_accuracy: 5.1644e-04 - past_key_values_3_accuracy: 5.0792e-04 - past_key_values_4_accuracy: 8.6128e-04 - past_key_values_5_accuracy: 5.4903e-04 - past_key_values_6_accuracy: 3.8467e-04 - past_key_values_7_accuracy: 5.1253e-04 - past_key_values_8_accuracy: 2.7348e-04 - past_key_values_9_accuracy: 4.7377e-04 - past_key_values_10_accuracy: 7.6124e-04 - past_key_values_11_accuracy: 6.9649e-04 - past_key_values_12_accuracy: 1.9943e-04\n",
      "Epoch 2/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 8.0747 - logits_loss: 8.0747 - logits_accuracy: 0.0666 - past_key_values_1_accuracy: 6.4182e-04 - past_key_values_2_accuracy: 5.9383e-04 - past_key_values_3_accuracy: 5.4945e-04 - past_key_values_4_accuracy: 6.6411e-04 - past_key_values_5_accuracy: 6.2578e-04 - past_key_values_6_accuracy: 6.2230e-04 - past_key_values_7_accuracy: 6.6000e-04 - past_key_values_8_accuracy: 4.4232e-04 - past_key_values_9_accuracy: 6.6262e-04 - past_key_values_10_accuracy: 5.3405e-04 - past_key_values_11_accuracy: 8.1080e-04 - past_key_values_12_accuracy: 3.0210e-04\n",
      "Epoch 3/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 7.8368 - logits_loss: 7.8368 - logits_accuracy: 0.0799 - past_key_values_1_accuracy: 6.5552e-04 - past_key_values_2_accuracy: 6.0384e-04 - past_key_values_3_accuracy: 5.7423e-04 - past_key_values_4_accuracy: 6.5922e-04 - past_key_values_5_accuracy: 6.5212e-04 - past_key_values_6_accuracy: 6.7711e-04 - past_key_values_7_accuracy: 6.3813e-04 - past_key_values_8_accuracy: 4.8513e-04 - past_key_values_9_accuracy: 7.0267e-04 - past_key_values_10_accuracy: 3.6259e-04 - past_key_values_11_accuracy: 7.8197e-04 - past_key_values_12_accuracy: 3.9262e-04\n",
      "Epoch 4/50\n",
      "494/494 [==============================] - 170s 345ms/step - loss: 7.6202 - logits_loss: 7.6202 - logits_accuracy: 0.0916 - past_key_values_1_accuracy: 6.2996e-04 - past_key_values_2_accuracy: 6.1342e-04 - past_key_values_3_accuracy: 5.7984e-04 - past_key_values_4_accuracy: 6.6525e-04 - past_key_values_5_accuracy: 6.5687e-04 - past_key_values_6_accuracy: 6.4963e-04 - past_key_values_7_accuracy: 5.2176e-04 - past_key_values_8_accuracy: 4.6440e-04 - past_key_values_9_accuracy: 6.3103e-04 - past_key_values_10_accuracy: 3.8651e-04 - past_key_values_11_accuracy: 6.8705e-04 - past_key_values_12_accuracy: 4.2783e-04\n",
      "Epoch 5/50\n",
      "494/494 [==============================] - 170s 344ms/step - loss: 7.4198 - logits_loss: 7.4198 - logits_accuracy: 0.1025 - past_key_values_1_accuracy: 6.2379e-04 - past_key_values_2_accuracy: 6.1875e-04 - past_key_values_3_accuracy: 5.9560e-04 - past_key_values_4_accuracy: 6.4551e-04 - past_key_values_5_accuracy: 6.4012e-04 - past_key_values_6_accuracy: 6.1513e-04 - past_key_values_7_accuracy: 4.8058e-04 - past_key_values_8_accuracy: 4.9407e-04 - past_key_values_9_accuracy: 5.8708e-04 - past_key_values_10_accuracy: 4.1697e-04 - past_key_values_11_accuracy: 6.9493e-04 - past_key_values_12_accuracy: 4.0568e-04\n",
      "Epoch 6/50\n",
      "494/494 [==============================] - 170s 344ms/step - loss: 7.2325 - logits_loss: 7.2325 - logits_accuracy: 0.1122 - past_key_values_1_accuracy: 6.1846e-04 - past_key_values_2_accuracy: 6.2230e-04 - past_key_values_3_accuracy: 6.1981e-04 - past_key_values_4_accuracy: 6.4019e-04 - past_key_values_5_accuracy: 6.4743e-04 - past_key_values_6_accuracy: 6.0845e-04 - past_key_values_7_accuracy: 4.8009e-04 - past_key_values_8_accuracy: 5.2240e-04 - past_key_values_9_accuracy: 5.9070e-04 - past_key_values_10_accuracy: 4.5978e-04 - past_key_values_11_accuracy: 6.4920e-04 - past_key_values_12_accuracy: 4.2954e-04\n",
      "Epoch 7/50\n",
      "494/494 [==============================] - 170s 344ms/step - loss: 7.0581 - logits_loss: 7.0581 - logits_accuracy: 0.1222 - past_key_values_1_accuracy: 6.2301e-04 - past_key_values_2_accuracy: 6.1612e-04 - past_key_values_3_accuracy: 6.0334e-04 - past_key_values_4_accuracy: 6.1747e-04 - past_key_values_5_accuracy: 6.5552e-04 - past_key_values_6_accuracy: 5.9383e-04 - past_key_values_7_accuracy: 4.6326e-04 - past_key_values_8_accuracy: 5.2872e-04 - past_key_values_9_accuracy: 5.6592e-04 - past_key_values_10_accuracy: 4.6198e-04 - past_key_values_11_accuracy: 6.3373e-04 - past_key_values_12_accuracy: 4.2173e-04\n",
      "Epoch 8/50\n",
      "494/494 [==============================] - 170s 344ms/step - loss: 6.8916 - logits_loss: 6.8916 - logits_accuracy: 0.1321 - past_key_values_1_accuracy: 5.9170e-04 - past_key_values_2_accuracy: 6.3032e-04 - past_key_values_3_accuracy: 6.1733e-04 - past_key_values_4_accuracy: 6.2585e-04 - past_key_values_5_accuracy: 6.7753e-04 - past_key_values_6_accuracy: 5.7423e-04 - past_key_values_7_accuracy: 4.6220e-04 - past_key_values_8_accuracy: 5.5428e-04 - past_key_values_9_accuracy: 5.6678e-04 - past_key_values_10_accuracy: 4.7299e-04 - past_key_values_11_accuracy: 6.3621e-04 - past_key_values_12_accuracy: 4.3756e-04\n",
      "Epoch 9/50\n",
      "494/494 [==============================] - 170s 344ms/step - loss: 6.7306 - logits_loss: 6.7306 - logits_accuracy: 0.1425 - past_key_values_1_accuracy: 6.0426e-04 - past_key_values_2_accuracy: 6.3678e-04 - past_key_values_3_accuracy: 6.1505e-04 - past_key_values_4_accuracy: 6.2485e-04 - past_key_values_5_accuracy: 6.6731e-04 - past_key_values_6_accuracy: 5.8069e-04 - past_key_values_7_accuracy: 4.7384e-04 - past_key_values_8_accuracy: 5.5953e-04 - past_key_values_9_accuracy: 5.7494e-04 - past_key_values_10_accuracy: 4.6944e-04 - past_key_values_11_accuracy: 6.4878e-04 - past_key_values_12_accuracy: 4.5701e-04\n",
      "Epoch 10/50\n",
      "494/494 [==============================] - 172s 348ms/step - loss: 6.5752 - logits_loss: 6.5752 - logits_accuracy: 0.1536 - past_key_values_1_accuracy: 6.0270e-04 - past_key_values_2_accuracy: 6.4871e-04 - past_key_values_3_accuracy: 6.1023e-04 - past_key_values_4_accuracy: 6.2720e-04 - past_key_values_5_accuracy: 6.7128e-04 - past_key_values_6_accuracy: 5.7636e-04 - past_key_values_7_accuracy: 4.8847e-04 - past_key_values_8_accuracy: 5.5826e-04 - past_key_values_9_accuracy: 5.8779e-04 - past_key_values_10_accuracy: 4.5538e-04 - past_key_values_11_accuracy: 6.5041e-04 - past_key_values_12_accuracy: 4.7973e-04\n",
      "Epoch 11/50\n",
      "494/494 [==============================] - 175s 354ms/step - loss: 6.4229 - logits_loss: 6.4229 - logits_accuracy: 0.1648 - past_key_values_1_accuracy: 6.0426e-04 - past_key_values_2_accuracy: 6.4622e-04 - past_key_values_3_accuracy: 6.3138e-04 - past_key_values_4_accuracy: 6.1051e-04 - past_key_values_5_accuracy: 6.7256e-04 - past_key_values_6_accuracy: 5.5343e-04 - past_key_values_7_accuracy: 4.8499e-04 - past_key_values_8_accuracy: 5.7423e-04 - past_key_values_9_accuracy: 5.8126e-04 - past_key_values_10_accuracy: 4.8172e-04 - past_key_values_11_accuracy: 6.1761e-04 - past_key_values_12_accuracy: 4.8690e-04\n",
      "Epoch 12/50\n",
      "494/494 [==============================] - 174s 351ms/step - loss: 6.2720 - logits_loss: 6.2720 - logits_accuracy: 0.1763 - past_key_values_1_accuracy: 5.9454e-04 - past_key_values_2_accuracy: 6.3997e-04 - past_key_values_3_accuracy: 6.4771e-04 - past_key_values_4_accuracy: 6.0377e-04 - past_key_values_5_accuracy: 6.7178e-04 - past_key_values_6_accuracy: 5.5833e-04 - past_key_values_7_accuracy: 4.9386e-04 - past_key_values_8_accuracy: 5.7785e-04 - past_key_values_9_accuracy: 5.8566e-04 - past_key_values_10_accuracy: 4.6979e-04 - past_key_values_11_accuracy: 6.1683e-04 - past_key_values_12_accuracy: 4.9060e-04\n",
      "Epoch 13/50\n",
      "494/494 [==============================] - 171s 345ms/step - loss: 6.1202 - logits_loss: 6.1202 - logits_accuracy: 0.1889 - past_key_values_1_accuracy: 5.9305e-04 - past_key_values_2_accuracy: 6.3479e-04 - past_key_values_3_accuracy: 6.2350e-04 - past_key_values_4_accuracy: 5.9099e-04 - past_key_values_5_accuracy: 6.8868e-04 - past_key_values_6_accuracy: 5.5776e-04 - past_key_values_7_accuracy: 4.9400e-04 - past_key_values_8_accuracy: 5.9865e-04 - past_key_values_9_accuracy: 5.9496e-04 - past_key_values_10_accuracy: 4.6525e-04 - past_key_values_11_accuracy: 6.0348e-04 - past_key_values_12_accuracy: 5.1481e-04\n",
      "Epoch 14/50\n",
      "494/494 [==============================] - 172s 348ms/step - loss: 5.9683 - logits_loss: 5.9683 - logits_accuracy: 0.2014 - past_key_values_1_accuracy: 5.8800e-04 - past_key_values_2_accuracy: 6.0810e-04 - past_key_values_3_accuracy: 6.3934e-04 - past_key_values_4_accuracy: 6.0128e-04 - past_key_values_5_accuracy: 6.8364e-04 - past_key_values_6_accuracy: 5.7778e-04 - past_key_values_7_accuracy: 4.8705e-04 - past_key_values_8_accuracy: 5.9922e-04 - past_key_values_9_accuracy: 5.9006e-04 - past_key_values_10_accuracy: 4.7938e-04 - past_key_values_11_accuracy: 6.2350e-04 - past_key_values_12_accuracy: 5.1665e-04\n",
      "Epoch 15/50\n",
      "494/494 [==============================] - 174s 352ms/step - loss: 5.8173 - logits_loss: 5.8173 - logits_accuracy: 0.2152 - past_key_values_1_accuracy: 5.9404e-04 - past_key_values_2_accuracy: 6.2720e-04 - past_key_values_3_accuracy: 6.3721e-04 - past_key_values_4_accuracy: 5.8630e-04 - past_key_values_5_accuracy: 7.0103e-04 - past_key_values_6_accuracy: 5.7409e-04 - past_key_values_7_accuracy: 4.9727e-04 - past_key_values_8_accuracy: 5.9269e-04 - past_key_values_9_accuracy: 5.9581e-04 - past_key_values_10_accuracy: 4.7348e-04 - past_key_values_11_accuracy: 6.0490e-04 - past_key_values_12_accuracy: 5.1481e-04\n",
      "Epoch 16/50\n",
      "494/494 [==============================] - 180s 364ms/step - loss: 5.6663 - logits_loss: 5.6663 - logits_accuracy: 0.2282 - past_key_values_1_accuracy: 5.8076e-04 - past_key_values_2_accuracy: 6.3273e-04 - past_key_values_3_accuracy: 6.4274e-04 - past_key_values_4_accuracy: 5.8012e-04 - past_key_values_5_accuracy: 7.2411e-04 - past_key_values_6_accuracy: 5.6017e-04 - past_key_values_7_accuracy: 4.8669e-04 - past_key_values_8_accuracy: 6.0206e-04 - past_key_values_9_accuracy: 6.0185e-04 - past_key_values_10_accuracy: 4.7285e-04 - past_key_values_11_accuracy: 6.0078e-04 - past_key_values_12_accuracy: 5.1204e-04\n",
      "Epoch 17/50\n",
      "494/494 [==============================] - 180s 364ms/step - loss: 5.5156 - logits_loss: 5.5156 - logits_accuracy: 0.2418 - past_key_values_1_accuracy: 5.8871e-04 - past_key_values_2_accuracy: 6.4381e-04 - past_key_values_3_accuracy: 6.3756e-04 - past_key_values_4_accuracy: 5.8666e-04 - past_key_values_5_accuracy: 6.9670e-04 - past_key_values_6_accuracy: 5.7111e-04 - past_key_values_7_accuracy: 4.8804e-04 - past_key_values_8_accuracy: 6.0320e-04 - past_key_values_9_accuracy: 6.0781e-04 - past_key_values_10_accuracy: 4.8676e-04 - past_key_values_11_accuracy: 5.9077e-04 - past_key_values_12_accuracy: 5.0011e-04\n",
      "Epoch 18/50\n",
      "494/494 [==============================] - 180s 364ms/step - loss: 5.3659 - logits_loss: 5.3659 - logits_accuracy: 0.2563 - past_key_values_1_accuracy: 5.9198e-04 - past_key_values_2_accuracy: 6.3437e-04 - past_key_values_3_accuracy: 6.3884e-04 - past_key_values_4_accuracy: 5.6358e-04 - past_key_values_5_accuracy: 6.8534e-04 - past_key_values_6_accuracy: 5.5854e-04 - past_key_values_7_accuracy: 5.0054e-04 - past_key_values_8_accuracy: 6.4736e-04 - past_key_values_9_accuracy: 6.3586e-04 - past_key_values_10_accuracy: 4.9109e-04 - past_key_values_11_accuracy: 6.0973e-04 - past_key_values_12_accuracy: 5.0763e-04\n",
      "Epoch 19/50\n",
      "494/494 [==============================] - 180s 364ms/step - loss: 5.2161 - logits_loss: 5.2161 - logits_accuracy: 0.2702 - past_key_values_1_accuracy: 5.9013e-04 - past_key_values_2_accuracy: 6.4984e-04 - past_key_values_3_accuracy: 6.3642e-04 - past_key_values_4_accuracy: 5.5904e-04 - past_key_values_5_accuracy: 6.7533e-04 - past_key_values_6_accuracy: 5.8112e-04 - past_key_values_7_accuracy: 4.9904e-04 - past_key_values_8_accuracy: 6.5488e-04 - past_key_values_9_accuracy: 6.6745e-04 - past_key_values_10_accuracy: 4.8144e-04 - past_key_values_11_accuracy: 6.0646e-04 - past_key_values_12_accuracy: 5.0714e-04\n",
      "Epoch 20/50\n",
      "494/494 [==============================] - 180s 364ms/step - loss: 5.0679 - logits_loss: 5.0679 - logits_accuracy: 0.2847 - past_key_values_1_accuracy: 5.8722e-04 - past_key_values_2_accuracy: 6.3479e-04 - past_key_values_3_accuracy: 6.3174e-04 - past_key_values_4_accuracy: 5.9049e-04 - past_key_values_5_accuracy: 6.6596e-04 - past_key_values_6_accuracy: 5.8559e-04 - past_key_values_7_accuracy: 4.9642e-04 - past_key_values_8_accuracy: 6.7675e-04 - past_key_values_9_accuracy: 6.6738e-04 - past_key_values_10_accuracy: 4.8903e-04 - past_key_values_11_accuracy: 6.0668e-04 - past_key_values_12_accuracy: 5.2737e-04\n",
      "Epoch 21/50\n",
      "494/494 [==============================] - 176s 357ms/step - loss: 4.9182 - logits_loss: 4.9182 - logits_accuracy: 0.2997 - past_key_values_1_accuracy: 5.7934e-04 - past_key_values_2_accuracy: 6.1811e-04 - past_key_values_3_accuracy: 6.4083e-04 - past_key_values_4_accuracy: 5.8069e-04 - past_key_values_5_accuracy: 6.7491e-04 - past_key_values_6_accuracy: 5.9972e-04 - past_key_values_7_accuracy: 4.8385e-04 - past_key_values_8_accuracy: 6.8094e-04 - past_key_values_9_accuracy: 6.6660e-04 - past_key_values_10_accuracy: 4.7803e-04 - past_key_values_11_accuracy: 6.0490e-04 - past_key_values_12_accuracy: 5.1985e-04\n",
      "Epoch 22/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 4.7743 - logits_loss: 4.7743 - logits_accuracy: 0.3143 - past_key_values_1_accuracy: 6.0128e-04 - past_key_values_2_accuracy: 6.4402e-04 - past_key_values_3_accuracy: 6.4991e-04 - past_key_values_4_accuracy: 5.6813e-04 - past_key_values_5_accuracy: 6.5112e-04 - past_key_values_6_accuracy: 5.7899e-04 - past_key_values_7_accuracy: 5.0217e-04 - past_key_values_8_accuracy: 6.8378e-04 - past_key_values_9_accuracy: 6.5389e-04 - past_key_values_10_accuracy: 4.9010e-04 - past_key_values_11_accuracy: 6.1626e-04 - past_key_values_12_accuracy: 5.1303e-04\n",
      "Epoch 23/50\n",
      "494/494 [==============================] - 172s 347ms/step - loss: 4.6273 - logits_loss: 4.6273 - logits_accuracy: 0.3289 - past_key_values_1_accuracy: 5.7849e-04 - past_key_values_2_accuracy: 6.4587e-04 - past_key_values_3_accuracy: 6.4395e-04 - past_key_values_4_accuracy: 5.7835e-04 - past_key_values_5_accuracy: 6.8556e-04 - past_key_values_6_accuracy: 5.9496e-04 - past_key_values_7_accuracy: 5.0487e-04 - past_key_values_8_accuracy: 7.0487e-04 - past_key_values_9_accuracy: 6.6440e-04 - past_key_values_10_accuracy: 5.0267e-04 - past_key_values_11_accuracy: 6.2357e-04 - past_key_values_12_accuracy: 5.2617e-04\n",
      "Epoch 24/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 4.4847 - logits_loss: 4.4847 - logits_accuracy: 0.3438 - past_key_values_1_accuracy: 5.8211e-04 - past_key_values_2_accuracy: 6.4409e-04 - past_key_values_3_accuracy: 6.5453e-04 - past_key_values_4_accuracy: 5.7075e-04 - past_key_values_5_accuracy: 6.7441e-04 - past_key_values_6_accuracy: 5.8524e-04 - past_key_values_7_accuracy: 5.0671e-04 - past_key_values_8_accuracy: 7.1190e-04 - past_key_values_9_accuracy: 6.5339e-04 - past_key_values_10_accuracy: 4.9081e-04 - past_key_values_11_accuracy: 6.1846e-04 - past_key_values_12_accuracy: 5.2325e-04\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494/494 [==============================] - 171s 346ms/step - loss: 4.3417 - logits_loss: 4.3417 - logits_accuracy: 0.3596 - past_key_values_1_accuracy: 5.7501e-04 - past_key_values_2_accuracy: 6.2244e-04 - past_key_values_3_accuracy: 6.5836e-04 - past_key_values_4_accuracy: 5.9198e-04 - past_key_values_5_accuracy: 6.7973e-04 - past_key_values_6_accuracy: 5.9603e-04 - past_key_values_7_accuracy: 5.1701e-04 - past_key_values_8_accuracy: 7.0373e-04 - past_key_values_9_accuracy: 6.5204e-04 - past_key_values_10_accuracy: 4.8555e-04 - past_key_values_11_accuracy: 6.2954e-04 - past_key_values_12_accuracy: 5.3348e-04\n",
      "Epoch 26/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 4.1986 - logits_loss: 4.1986 - logits_accuracy: 0.3759 - past_key_values_1_accuracy: 5.8389e-04 - past_key_values_2_accuracy: 6.4622e-04 - past_key_values_3_accuracy: 6.7150e-04 - past_key_values_4_accuracy: 5.8232e-04 - past_key_values_5_accuracy: 6.6738e-04 - past_key_values_6_accuracy: 5.9830e-04 - past_key_values_7_accuracy: 5.1658e-04 - past_key_values_8_accuracy: 6.9862e-04 - past_key_values_9_accuracy: 6.6553e-04 - past_key_values_10_accuracy: 4.9528e-04 - past_key_values_11_accuracy: 6.2308e-04 - past_key_values_12_accuracy: 5.2858e-04\n",
      "Epoch 27/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 4.0604 - logits_loss: 4.0604 - logits_accuracy: 0.3909 - past_key_values_1_accuracy: 6.0135e-04 - past_key_values_2_accuracy: 6.5290e-04 - past_key_values_3_accuracy: 6.4402e-04 - past_key_values_4_accuracy: 5.7231e-04 - past_key_values_5_accuracy: 6.7789e-04 - past_key_values_6_accuracy: 6.1179e-04 - past_key_values_7_accuracy: 5.1126e-04 - past_key_values_8_accuracy: 7.2439e-04 - past_key_values_9_accuracy: 6.5687e-04 - past_key_values_10_accuracy: 5.1523e-04 - past_key_values_11_accuracy: 6.2471e-04 - past_key_values_12_accuracy: 5.4363e-04\n",
      "Epoch 28/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 3.9217 - logits_loss: 3.9217 - logits_accuracy: 0.4072 - past_key_values_1_accuracy: 5.8474e-04 - past_key_values_2_accuracy: 6.4849e-04 - past_key_values_3_accuracy: 6.6475e-04 - past_key_values_4_accuracy: 5.6458e-04 - past_key_values_5_accuracy: 6.5645e-04 - past_key_values_6_accuracy: 6.2372e-04 - past_key_values_7_accuracy: 5.0948e-04 - past_key_values_8_accuracy: 7.3909e-04 - past_key_values_9_accuracy: 6.6937e-04 - past_key_values_10_accuracy: 5.0920e-04 - past_key_values_11_accuracy: 6.3628e-04 - past_key_values_12_accuracy: 5.3504e-04\n",
      "Epoch 29/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 3.7831 - logits_loss: 3.7831 - logits_accuracy: 0.4236 - past_key_values_1_accuracy: 5.7558e-04 - past_key_values_2_accuracy: 6.4033e-04 - past_key_values_3_accuracy: 6.6880e-04 - past_key_values_4_accuracy: 5.6813e-04 - past_key_values_5_accuracy: 6.5907e-04 - past_key_values_6_accuracy: 6.0653e-04 - past_key_values_7_accuracy: 5.2993e-04 - past_key_values_8_accuracy: 7.4853e-04 - past_key_values_9_accuracy: 6.6447e-04 - past_key_values_10_accuracy: 5.0529e-04 - past_key_values_11_accuracy: 6.2776e-04 - past_key_values_12_accuracy: 5.2205e-04\n",
      "Epoch 30/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 3.6483 - logits_loss: 3.6483 - logits_accuracy: 0.4395 - past_key_values_1_accuracy: 5.8893e-04 - past_key_values_2_accuracy: 6.4913e-04 - past_key_values_3_accuracy: 6.7604e-04 - past_key_values_4_accuracy: 5.8275e-04 - past_key_values_5_accuracy: 6.4509e-04 - past_key_values_6_accuracy: 6.0838e-04 - past_key_values_7_accuracy: 5.3831e-04 - past_key_values_8_accuracy: 7.5165e-04 - past_key_values_9_accuracy: 6.7959e-04 - past_key_values_10_accuracy: 5.2624e-04 - past_key_values_11_accuracy: 6.2613e-04 - past_key_values_12_accuracy: 5.4044e-04\n",
      "Epoch 31/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 3.5147 - logits_loss: 3.5147 - logits_accuracy: 0.4561 - past_key_values_1_accuracy: 5.8268e-04 - past_key_values_2_accuracy: 6.5716e-04 - past_key_values_3_accuracy: 6.6000e-04 - past_key_values_4_accuracy: 5.7437e-04 - past_key_values_5_accuracy: 6.4047e-04 - past_key_values_6_accuracy: 6.0696e-04 - past_key_values_7_accuracy: 5.2574e-04 - past_key_values_8_accuracy: 7.5748e-04 - past_key_values_9_accuracy: 6.6986e-04 - past_key_values_10_accuracy: 5.2482e-04 - past_key_values_11_accuracy: 6.2819e-04 - past_key_values_12_accuracy: 5.3639e-04\n",
      "Epoch 32/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 3.3803 - logits_loss: 3.3803 - logits_accuracy: 0.4737 - past_key_values_1_accuracy: 5.7700e-04 - past_key_values_2_accuracy: 6.2812e-04 - past_key_values_3_accuracy: 6.6156e-04 - past_key_values_4_accuracy: 5.7849e-04 - past_key_values_5_accuracy: 6.3238e-04 - past_key_values_6_accuracy: 6.0888e-04 - past_key_values_7_accuracy: 5.5002e-04 - past_key_values_8_accuracy: 7.3249e-04 - past_key_values_9_accuracy: 6.8044e-04 - past_key_values_10_accuracy: 5.2609e-04 - past_key_values_11_accuracy: 6.2663e-04 - past_key_values_12_accuracy: 5.5137e-04\n",
      "Epoch 33/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 3.2509 - logits_loss: 3.2509 - logits_accuracy: 0.4905 - past_key_values_1_accuracy: 5.7018e-04 - past_key_values_2_accuracy: 6.3735e-04 - past_key_values_3_accuracy: 6.5744e-04 - past_key_values_4_accuracy: 5.8133e-04 - past_key_values_5_accuracy: 6.3075e-04 - past_key_values_6_accuracy: 6.0213e-04 - past_key_values_7_accuracy: 5.3064e-04 - past_key_values_8_accuracy: 7.5449e-04 - past_key_values_9_accuracy: 6.8122e-04 - past_key_values_10_accuracy: 5.2127e-04 - past_key_values_11_accuracy: 6.2968e-04 - past_key_values_12_accuracy: 5.3312e-04\n",
      "Epoch 34/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 3.1215 - logits_loss: 3.1215 - logits_accuracy: 0.5081 - past_key_values_1_accuracy: 5.7295e-04 - past_key_values_2_accuracy: 6.3202e-04 - past_key_values_3_accuracy: 6.7533e-04 - past_key_values_4_accuracy: 5.8488e-04 - past_key_values_5_accuracy: 6.4587e-04 - past_key_values_6_accuracy: 6.1804e-04 - past_key_values_7_accuracy: 5.3738e-04 - past_key_values_8_accuracy: 7.4406e-04 - past_key_values_9_accuracy: 6.8151e-04 - past_key_values_10_accuracy: 5.3341e-04 - past_key_values_11_accuracy: 6.3103e-04 - past_key_values_12_accuracy: 5.4924e-04\n",
      "Epoch 35/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 2.9933 - logits_loss: 2.9933 - logits_accuracy: 0.5255 - past_key_values_1_accuracy: 5.8481e-04 - past_key_values_2_accuracy: 6.3628e-04 - past_key_values_3_accuracy: 6.7221e-04 - past_key_values_4_accuracy: 5.6791e-04 - past_key_values_5_accuracy: 6.2975e-04 - past_key_values_6_accuracy: 6.1058e-04 - past_key_values_7_accuracy: 5.3717e-04 - past_key_values_8_accuracy: 7.5442e-04 - past_key_values_9_accuracy: 6.7292e-04 - past_key_values_10_accuracy: 5.3525e-04 - past_key_values_11_accuracy: 6.3011e-04 - past_key_values_12_accuracy: 5.3845e-04\n",
      "Epoch 36/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 2.8664 - logits_loss: 2.8664 - logits_accuracy: 0.5444 - past_key_values_1_accuracy: 5.7139e-04 - past_key_values_2_accuracy: 6.3983e-04 - past_key_values_3_accuracy: 6.6248e-04 - past_key_values_4_accuracy: 5.8971e-04 - past_key_values_5_accuracy: 6.1065e-04 - past_key_values_6_accuracy: 6.2428e-04 - past_key_values_7_accuracy: 5.3603e-04 - past_key_values_8_accuracy: 7.6351e-04 - past_key_values_9_accuracy: 6.8094e-04 - past_key_values_10_accuracy: 5.4257e-04 - past_key_values_11_accuracy: 6.2862e-04 - past_key_values_12_accuracy: 5.3880e-04\n",
      "Epoch 37/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.7399 - logits_loss: 2.7399 - logits_accuracy: 0.5634 - past_key_values_1_accuracy: 5.8062e-04 - past_key_values_2_accuracy: 6.2904e-04 - past_key_values_3_accuracy: 6.7988e-04 - past_key_values_4_accuracy: 5.7366e-04 - past_key_values_5_accuracy: 6.0987e-04 - past_key_values_6_accuracy: 6.1683e-04 - past_key_values_7_accuracy: 5.5698e-04 - past_key_values_8_accuracy: 7.6578e-04 - past_key_values_9_accuracy: 6.7292e-04 - past_key_values_10_accuracy: 5.5513e-04 - past_key_values_11_accuracy: 6.2173e-04 - past_key_values_12_accuracy: 5.3490e-04\n",
      "Epoch 38/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.6167 - logits_loss: 2.6167 - logits_accuracy: 0.5816 - past_key_values_1_accuracy: 5.7970e-04 - past_key_values_2_accuracy: 6.3288e-04 - past_key_values_3_accuracy: 6.6653e-04 - past_key_values_4_accuracy: 5.8808e-04 - past_key_values_5_accuracy: 6.3486e-04 - past_key_values_6_accuracy: 6.2407e-04 - past_key_values_7_accuracy: 5.3816e-04 - past_key_values_8_accuracy: 7.5222e-04 - past_key_values_9_accuracy: 6.8073e-04 - past_key_values_10_accuracy: 5.5293e-04 - past_key_values_11_accuracy: 6.2691e-04 - past_key_values_12_accuracy: 5.5655e-04\n",
      "Epoch 39/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.4966 - logits_loss: 2.4966 - logits_accuracy: 0.6005 - past_key_values_1_accuracy: 5.8303e-04 - past_key_values_2_accuracy: 6.2670e-04 - past_key_values_3_accuracy: 6.6979e-04 - past_key_values_4_accuracy: 5.7984e-04 - past_key_values_5_accuracy: 6.2940e-04 - past_key_values_6_accuracy: 6.1662e-04 - past_key_values_7_accuracy: 5.3277e-04 - past_key_values_8_accuracy: 7.5776e-04 - past_key_values_9_accuracy: 6.7682e-04 - past_key_values_10_accuracy: 5.4391e-04 - past_key_values_11_accuracy: 6.2137e-04 - past_key_values_12_accuracy: 5.4065e-04\n",
      "Epoch 40/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.3749 - logits_loss: 2.3749 - logits_accuracy: 0.6205 - past_key_values_1_accuracy: 5.8119e-04 - past_key_values_2_accuracy: 6.3209e-04 - past_key_values_3_accuracy: 6.6994e-04 - past_key_values_4_accuracy: 5.7657e-04 - past_key_values_5_accuracy: 6.4033e-04 - past_key_values_6_accuracy: 6.1513e-04 - past_key_values_7_accuracy: 5.3483e-04 - past_key_values_8_accuracy: 7.7558e-04 - past_key_values_9_accuracy: 7.0309e-04 - past_key_values_10_accuracy: 5.4952e-04 - past_key_values_11_accuracy: 6.2428e-04 - past_key_values_12_accuracy: 5.5130e-04\n",
      "Epoch 41/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.2616 - logits_loss: 2.2616 - logits_accuracy: 0.6395 - past_key_values_1_accuracy: 5.8680e-04 - past_key_values_2_accuracy: 6.2769e-04 - past_key_values_3_accuracy: 6.6326e-04 - past_key_values_4_accuracy: 5.7998e-04 - past_key_values_5_accuracy: 6.4189e-04 - past_key_values_6_accuracy: 6.2634e-04 - past_key_values_7_accuracy: 5.5549e-04 - past_key_values_8_accuracy: 7.7011e-04 - past_key_values_9_accuracy: 6.8598e-04 - past_key_values_10_accuracy: 5.5961e-04 - past_key_values_11_accuracy: 6.1946e-04 - past_key_values_12_accuracy: 5.5875e-04\n",
      "Epoch 42/50\n",
      "494/494 [==============================] - 172s 347ms/step - loss: 2.1407 - logits_loss: 2.1407 - logits_accuracy: 0.6599 - past_key_values_1_accuracy: 5.9269e-04 - past_key_values_2_accuracy: 6.2798e-04 - past_key_values_3_accuracy: 6.7810e-04 - past_key_values_4_accuracy: 5.6216e-04 - past_key_values_5_accuracy: 6.3657e-04 - past_key_values_6_accuracy: 6.1797e-04 - past_key_values_7_accuracy: 5.5123e-04 - past_key_values_8_accuracy: 7.8091e-04 - past_key_values_9_accuracy: 6.8520e-04 - past_key_values_10_accuracy: 5.5144e-04 - past_key_values_11_accuracy: 6.1477e-04 - past_key_values_12_accuracy: 5.6294e-04\n",
      "Epoch 43/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 2.0258 - logits_loss: 2.0258 - logits_accuracy: 0.6808 - past_key_values_1_accuracy: 5.8169e-04 - past_key_values_2_accuracy: 6.3245e-04 - past_key_values_3_accuracy: 6.8399e-04 - past_key_values_4_accuracy: 5.7459e-04 - past_key_values_5_accuracy: 6.5361e-04 - past_key_values_6_accuracy: 6.2507e-04 - past_key_values_7_accuracy: 5.4888e-04 - past_key_values_8_accuracy: 7.7274e-04 - past_key_values_9_accuracy: 6.7789e-04 - past_key_values_10_accuracy: 5.3965e-04 - past_key_values_11_accuracy: 6.1008e-04 - past_key_values_12_accuracy: 5.5918e-04\n",
      "Epoch 44/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 1.9154 - logits_loss: 1.9154 - logits_accuracy: 0.6999 - past_key_values_1_accuracy: 5.7466e-04 - past_key_values_2_accuracy: 6.2585e-04 - past_key_values_3_accuracy: 6.7569e-04 - past_key_values_4_accuracy: 5.7210e-04 - past_key_values_5_accuracy: 6.2883e-04 - past_key_values_6_accuracy: 6.2379e-04 - past_key_values_7_accuracy: 5.4455e-04 - past_key_values_8_accuracy: 7.7366e-04 - past_key_values_9_accuracy: 6.8343e-04 - past_key_values_10_accuracy: 5.3575e-04 - past_key_values_11_accuracy: 6.0071e-04 - past_key_values_12_accuracy: 5.5826e-04\n",
      "Epoch 45/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 1.8071 - logits_loss: 1.8071 - logits_accuracy: 0.7198 - past_key_values_1_accuracy: 5.7380e-04 - past_key_values_2_accuracy: 6.3486e-04 - past_key_values_3_accuracy: 6.8130e-04 - past_key_values_4_accuracy: 5.7011e-04 - past_key_values_5_accuracy: 6.3437e-04 - past_key_values_6_accuracy: 6.2507e-04 - past_key_values_7_accuracy: 5.4597e-04 - past_key_values_8_accuracy: 7.8147e-04 - past_key_values_9_accuracy: 6.9748e-04 - past_key_values_10_accuracy: 5.5243e-04 - past_key_values_11_accuracy: 6.1761e-04 - past_key_values_12_accuracy: 5.5499e-04\n",
      "Epoch 46/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 1.7000 - logits_loss: 1.7000 - logits_accuracy: 0.7397 - past_key_values_1_accuracy: 5.7459e-04 - past_key_values_2_accuracy: 6.3011e-04 - past_key_values_3_accuracy: 6.9209e-04 - past_key_values_4_accuracy: 5.7118e-04 - past_key_values_5_accuracy: 6.3777e-04 - past_key_values_6_accuracy: 6.2073e-04 - past_key_values_7_accuracy: 5.3603e-04 - past_key_values_8_accuracy: 7.7814e-04 - past_key_values_9_accuracy: 7.0196e-04 - past_key_values_10_accuracy: 5.4157e-04 - past_key_values_11_accuracy: 6.2471e-04 - past_key_values_12_accuracy: 5.7430e-04\n",
      "Epoch 47/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 1.5948 - logits_loss: 1.5948 - logits_accuracy: 0.7595 - past_key_values_1_accuracy: 5.7991e-04 - past_key_values_2_accuracy: 6.3919e-04 - past_key_values_3_accuracy: 6.8037e-04 - past_key_values_4_accuracy: 5.5946e-04 - past_key_values_5_accuracy: 6.3699e-04 - past_key_values_6_accuracy: 6.3529e-04 - past_key_values_7_accuracy: 5.1750e-04 - past_key_values_8_accuracy: 7.8367e-04 - past_key_values_9_accuracy: 6.8683e-04 - past_key_values_10_accuracy: 5.5669e-04 - past_key_values_11_accuracy: 6.1420e-04 - past_key_values_12_accuracy: 5.6869e-04\n",
      "Epoch 48/50\n",
      "494/494 [==============================] - 171s 346ms/step - loss: 1.4947 - logits_loss: 1.4947 - logits_accuracy: 0.7783 - past_key_values_1_accuracy: 5.7821e-04 - past_key_values_2_accuracy: 6.2237e-04 - past_key_values_3_accuracy: 6.7824e-04 - past_key_values_4_accuracy: 5.9148e-04 - past_key_values_5_accuracy: 6.3926e-04 - past_key_values_6_accuracy: 6.3238e-04 - past_key_values_7_accuracy: 5.5378e-04 - past_key_values_8_accuracy: 7.6869e-04 - past_key_values_9_accuracy: 7.0600e-04 - past_key_values_10_accuracy: 5.4910e-04 - past_key_values_11_accuracy: 6.1711e-04 - past_key_values_12_accuracy: 5.6692e-04\n",
      "Epoch 49/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 1.3974 - logits_loss: 1.3974 - logits_accuracy: 0.7962 - past_key_values_1_accuracy: 5.7593e-04 - past_key_values_2_accuracy: 6.3863e-04 - past_key_values_3_accuracy: 6.9479e-04 - past_key_values_4_accuracy: 5.7686e-04 - past_key_values_5_accuracy: 6.3877e-04 - past_key_values_6_accuracy: 6.1768e-04 - past_key_values_7_accuracy: 5.4988e-04 - past_key_values_8_accuracy: 7.7288e-04 - past_key_values_9_accuracy: 6.8264e-04 - past_key_values_10_accuracy: 5.5172e-04 - past_key_values_11_accuracy: 6.1342e-04 - past_key_values_12_accuracy: 5.6372e-04\n",
      "Epoch 50/50\n",
      "494/494 [==============================] - 171s 347ms/step - loss: 1.3003 - logits_loss: 1.3003 - logits_accuracy: 0.8139 - past_key_values_1_accuracy: 5.7757e-04 - past_key_values_2_accuracy: 6.2386e-04 - past_key_values_3_accuracy: 6.7909e-04 - past_key_values_4_accuracy: 5.7636e-04 - past_key_values_5_accuracy: 6.3309e-04 - past_key_values_6_accuracy: 6.2869e-04 - past_key_values_7_accuracy: 5.1772e-04 - past_key_values_8_accuracy: 7.7998e-04 - past_key_values_9_accuracy: 6.8257e-04 - past_key_values_10_accuracy: 5.5698e-04 - past_key_values_11_accuracy: 6.2045e-04 - past_key_values_12_accuracy: 5.5875e-04\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 50\n",
    "history = model.fit(dataset, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "text = \" كتاب وحش\"\n",
    "# encoding the input text\n",
    "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "# getting out output\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    min_length=10,\n",
    "    top_k=40,\n",
    "    num_return_sequences=5  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " كتاب وحش. بجد :)\"\n",
      "\"بعض المقالات تحمل رسائل صغيرة.ـيدة غريبة رومانسية تخنق للأبل هل ستجنبها الج نوبل على التلفاز لكني لم تستخذ من جسد الزمن. حاولت أنى أقرأ ماذا تمر بهذا الشكل لأني لم تكن بالقدر او فلسفة شديدة الا\n",
      "============================\n",
      " كتاب وحش. لكن بطريقة عام لم تكن بالمستوى المطلوب! كان مجموعة قصص قصيرة مفيدة. كُقل من الصحة كأن الكتابة قبل قبل القروليتي على العموم طبعاً جدًا مما يجعلها الأخطاء الكبيرة التي لا أستطيع أن أُقرأ في الحياة.\"\n",
      "============================\n",
      " كتاب وحش. جزء كبير ملوش اي شي جديد.\"\n",
      "\"قرأته في جلسة واحدة و لن تخرج منه لكن أحداثها ذكي نظرا بالغثيان في انتظار الصحف بتاعها لم استفد منه\"\n",
      "\"في معظم الحالات العربيه ثم أتممت قراءة الكتاب.\"\n",
      "\"لم أجد فيها\n",
      "============================\n",
      " كتاب وحش.بس بجد :)\"\n",
      "\"الكتاب يعيد قصص قصيرة ولكن معظمه قائم على الانترنت. قرأته فعلاً. لم يرق لي متعة القراءة لاحقا ل\"\n",
      "\"اول مرة اكمله\"\n",
      "\"بعض المقالات يتحدث عن مجموعة مقالات احمد حلمى بيحاول يبقى في طفولتي و\n",
      "============================\n",
      " كتاب وحش. لكن واقع تاريخ نصائح جديدة متشابهة للغاية\"\n",
      "\"الكتاب صغير جداً, تحس مجهود واضح, بناء على عدد صفحات عالية, مجموعة مقالات ساخرة ربما كان السبب اللي يستحق القراءه مجرد اجزاء مختلفة.\"\n",
      "\"الكتاب ليس اكثر من مقال ان أسلوب الكاتب\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "for sentence in beam_output:\n",
    "    print(tokenizer.decode(sentence))\n",
    "    print('============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt-2-negative-reviews/tokenizer_config.json',\n",
       " './gpt-2-negative-reviews/special_tokens_map.json',\n",
       " './gpt-2-negative-reviews/vocab.json',\n",
       " './gpt-2-negative-reviews/merges.txt',\n",
       " './gpt-2-negative-reviews/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "import os\n",
    "output_dir = './gpt-2-negative-reviews/'\n",
    "# creating directory if it is not present\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "# save model and model configs\n",
    "model.save_pretrained(output_dir)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt-2-negative-reviews/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the model to work with pytorch also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pt_model = GPT2LMHeadModel.from_pretrained(output_dir, from_tf=True)\n",
    "pt_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50000, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained(\"gpt-2-negative-reviews\")\n",
    "AutoModel.from_pretrained(\"gpt-2-negative-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f853eeac1f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
